/*
 * Copyright 2025 Google LLC
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     https://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: google/cloud/dialogflow/v2/generator.proto

// Protobuf Java Version: 3.25.8
package com.google.cloud.dialogflow.v2;

public interface InferenceParameterOrBuilder
    extends
    // @@protoc_insertion_point(interface_extends:google.cloud.dialogflow.v2.InferenceParameter)
    com.google.protobuf.MessageOrBuilder {

  /**
   *
   *
   * <pre>
   * Optional. Maximum number of the output tokens for the generator.
   * </pre>
   *
   * <code>optional int32 max_output_tokens = 1 [(.google.api.field_behavior) = OPTIONAL];</code>
   *
   * @return Whether the maxOutputTokens field is set.
   */
  boolean hasMaxOutputTokens();

  /**
   *
   *
   * <pre>
   * Optional. Maximum number of the output tokens for the generator.
   * </pre>
   *
   * <code>optional int32 max_output_tokens = 1 [(.google.api.field_behavior) = OPTIONAL];</code>
   *
   * @return The maxOutputTokens.
   */
  int getMaxOutputTokens();

  /**
   *
   *
   * <pre>
   * Optional. Controls the randomness of LLM predictions.
   * Low temperature = less random. High temperature = more random.
   * If unset (or 0), uses a default value of 0.
   * </pre>
   *
   * <code>optional double temperature = 2 [(.google.api.field_behavior) = OPTIONAL];</code>
   *
   * @return Whether the temperature field is set.
   */
  boolean hasTemperature();

  /**
   *
   *
   * <pre>
   * Optional. Controls the randomness of LLM predictions.
   * Low temperature = less random. High temperature = more random.
   * If unset (or 0), uses a default value of 0.
   * </pre>
   *
   * <code>optional double temperature = 2 [(.google.api.field_behavior) = OPTIONAL];</code>
   *
   * @return The temperature.
   */
  double getTemperature();

  /**
   *
   *
   * <pre>
   * Optional. Top-k changes how the model selects tokens for output. A top-k of
   * 1 means the selected token is the most probable among all tokens in the
   * model's vocabulary (also called greedy decoding), while a top-k of 3 means
   * that the next token is selected from among the 3 most probable tokens
   * (using temperature). For each token selection step, the top K tokens with
   * the highest probabilities are sampled. Then tokens are further filtered
   * based on topP with the final token selected using temperature sampling.
   * Specify a lower value for less random responses and a higher value for more
   * random responses. Acceptable value is [1, 40], default to 40.
   * </pre>
   *
   * <code>optional int32 top_k = 3 [(.google.api.field_behavior) = OPTIONAL];</code>
   *
   * @return Whether the topK field is set.
   */
  boolean hasTopK();

  /**
   *
   *
   * <pre>
   * Optional. Top-k changes how the model selects tokens for output. A top-k of
   * 1 means the selected token is the most probable among all tokens in the
   * model's vocabulary (also called greedy decoding), while a top-k of 3 means
   * that the next token is selected from among the 3 most probable tokens
   * (using temperature). For each token selection step, the top K tokens with
   * the highest probabilities are sampled. Then tokens are further filtered
   * based on topP with the final token selected using temperature sampling.
   * Specify a lower value for less random responses and a higher value for more
   * random responses. Acceptable value is [1, 40], default to 40.
   * </pre>
   *
   * <code>optional int32 top_k = 3 [(.google.api.field_behavior) = OPTIONAL];</code>
   *
   * @return The topK.
   */
  int getTopK();

  /**
   *
   *
   * <pre>
   * Optional. Top-p changes how the model selects tokens for output. Tokens are
   * selected from most K (see topK parameter) probable to least until the sum
   * of their probabilities equals the top-p value. For example, if tokens A, B,
   * and C have a probability of 0.3, 0.2, and 0.1 and the top-p value is 0.5,
   * then the model will select either A or B as the next token (using
   * temperature) and doesn't consider C. The default top-p value is 0.95.
   * Specify a lower value for less random responses and a higher value for more
   * random responses. Acceptable value is [0.0, 1.0], default to 0.95.
   * </pre>
   *
   * <code>optional double top_p = 4 [(.google.api.field_behavior) = OPTIONAL];</code>
   *
   * @return Whether the topP field is set.
   */
  boolean hasTopP();

  /**
   *
   *
   * <pre>
   * Optional. Top-p changes how the model selects tokens for output. Tokens are
   * selected from most K (see topK parameter) probable to least until the sum
   * of their probabilities equals the top-p value. For example, if tokens A, B,
   * and C have a probability of 0.3, 0.2, and 0.1 and the top-p value is 0.5,
   * then the model will select either A or B as the next token (using
   * temperature) and doesn't consider C. The default top-p value is 0.95.
   * Specify a lower value for less random responses and a higher value for more
   * random responses. Acceptable value is [0.0, 1.0], default to 0.95.
   * </pre>
   *
   * <code>optional double top_p = 4 [(.google.api.field_behavior) = OPTIONAL];</code>
   *
   * @return The topP.
   */
  double getTopP();
}
