/*
 * Copyright 2025 Google LLC
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     https://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: google/cloud/aiplatform/v1beta1/model_monitor.proto

// Protobuf Java Version: 3.25.5
package com.google.cloud.aiplatform.v1beta1;

/**
 *
 *
 * <pre>
 * The Model Monitoring Schema definition.
 * </pre>
 *
 * Protobuf type {@code google.cloud.aiplatform.v1beta1.ModelMonitoringSchema}
 */
public final class ModelMonitoringSchema extends com.google.protobuf.GeneratedMessageV3
    implements
    // @@protoc_insertion_point(message_implements:google.cloud.aiplatform.v1beta1.ModelMonitoringSchema)
    ModelMonitoringSchemaOrBuilder {
  private static final long serialVersionUID = 0L;
  // Use ModelMonitoringSchema.newBuilder() to construct.
  private ModelMonitoringSchema(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
    super(builder);
  }

  private ModelMonitoringSchema() {
    featureFields_ = java.util.Collections.emptyList();
    predictionFields_ = java.util.Collections.emptyList();
    groundTruthFields_ = java.util.Collections.emptyList();
  }

  @java.lang.Override
  @SuppressWarnings({"unused"})
  protected java.lang.Object newInstance(UnusedPrivateParameter unused) {
    return new ModelMonitoringSchema();
  }

  public static final com.google.protobuf.Descriptors.Descriptor getDescriptor() {
    return com.google.cloud.aiplatform.v1beta1.ModelMonitorProto
        .internal_static_google_cloud_aiplatform_v1beta1_ModelMonitoringSchema_descriptor;
  }

  @java.lang.Override
  protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internalGetFieldAccessorTable() {
    return com.google.cloud.aiplatform.v1beta1.ModelMonitorProto
        .internal_static_google_cloud_aiplatform_v1beta1_ModelMonitoringSchema_fieldAccessorTable
        .ensureFieldAccessorsInitialized(
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.class,
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.Builder.class);
  }

  public interface FieldSchemaOrBuilder
      extends
      // @@protoc_insertion_point(interface_extends:google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema)
      com.google.protobuf.MessageOrBuilder {

    /**
     *
     *
     * <pre>
     * Field name.
     * </pre>
     *
     * <code>string name = 1;</code>
     *
     * @return The name.
     */
    java.lang.String getName();
    /**
     *
     *
     * <pre>
     * Field name.
     * </pre>
     *
     * <code>string name = 1;</code>
     *
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString getNameBytes();

    /**
     *
     *
     * <pre>
     * Supported data types are:
     * `float`
     * `integer`
     * `boolean`
     * `string`
     * `categorical`
     * </pre>
     *
     * <code>string data_type = 2;</code>
     *
     * @return The dataType.
     */
    java.lang.String getDataType();
    /**
     *
     *
     * <pre>
     * Supported data types are:
     * `float`
     * `integer`
     * `boolean`
     * `string`
     * `categorical`
     * </pre>
     *
     * <code>string data_type = 2;</code>
     *
     * @return The bytes for dataType.
     */
    com.google.protobuf.ByteString getDataTypeBytes();

    /**
     *
     *
     * <pre>
     * Describes if the schema field is an array of given data type.
     * </pre>
     *
     * <code>bool repeated = 3;</code>
     *
     * @return The repeated.
     */
    boolean getRepeated();
  }
  /**
   *
   *
   * <pre>
   * Schema field definition.
   * </pre>
   *
   * Protobuf type {@code google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema}
   */
  public static final class FieldSchema extends com.google.protobuf.GeneratedMessageV3
      implements
      // @@protoc_insertion_point(message_implements:google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema)
      FieldSchemaOrBuilder {
    private static final long serialVersionUID = 0L;
    // Use FieldSchema.newBuilder() to construct.
    private FieldSchema(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }

    private FieldSchema() {
      name_ = "";
      dataType_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(UnusedPrivateParameter unused) {
      return new FieldSchema();
    }

    public static final com.google.protobuf.Descriptors.Descriptor getDescriptor() {
      return com.google.cloud.aiplatform.v1beta1.ModelMonitorProto
          .internal_static_google_cloud_aiplatform_v1beta1_ModelMonitoringSchema_FieldSchema_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return com.google.cloud.aiplatform.v1beta1.ModelMonitorProto
          .internal_static_google_cloud_aiplatform_v1beta1_ModelMonitoringSchema_FieldSchema_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.class,
              com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder.class);
    }

    public static final int NAME_FIELD_NUMBER = 1;

    @SuppressWarnings("serial")
    private volatile java.lang.Object name_ = "";
    /**
     *
     *
     * <pre>
     * Field name.
     * </pre>
     *
     * <code>string name = 1;</code>
     *
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     *
     *
     * <pre>
     * Field name.
     * </pre>
     *
     * <code>string name = 1;</code>
     *
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b =
            com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int DATA_TYPE_FIELD_NUMBER = 2;

    @SuppressWarnings("serial")
    private volatile java.lang.Object dataType_ = "";
    /**
     *
     *
     * <pre>
     * Supported data types are:
     * `float`
     * `integer`
     * `boolean`
     * `string`
     * `categorical`
     * </pre>
     *
     * <code>string data_type = 2;</code>
     *
     * @return The dataType.
     */
    @java.lang.Override
    public java.lang.String getDataType() {
      java.lang.Object ref = dataType_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        dataType_ = s;
        return s;
      }
    }
    /**
     *
     *
     * <pre>
     * Supported data types are:
     * `float`
     * `integer`
     * `boolean`
     * `string`
     * `categorical`
     * </pre>
     *
     * <code>string data_type = 2;</code>
     *
     * @return The bytes for dataType.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString getDataTypeBytes() {
      java.lang.Object ref = dataType_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b =
            com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);
        dataType_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int REPEATED_FIELD_NUMBER = 3;
    private boolean repeated_ = false;
    /**
     *
     *
     * <pre>
     * Describes if the schema field is an array of given data type.
     * </pre>
     *
     * <code>bool repeated = 3;</code>
     *
     * @return The repeated.
     */
    @java.lang.Override
    public boolean getRepeated() {
      return repeated_;
    }

    private byte memoizedIsInitialized = -1;

    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output) throws java.io.IOException {
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(name_)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(dataType_)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, dataType_);
      }
      if (repeated_ != false) {
        output.writeBool(3, repeated_);
      }
      getUnknownFields().writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(name_)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(dataType_)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, dataType_);
      }
      if (repeated_ != false) {
        size += com.google.protobuf.CodedOutputStream.computeBoolSize(3, repeated_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
        return true;
      }
      if (!(obj instanceof com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema)) {
        return super.equals(obj);
      }
      com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema other =
          (com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema) obj;

      if (!getName().equals(other.getName())) return false;
      if (!getDataType().equals(other.getDataType())) return false;
      if (getRepeated() != other.getRepeated()) return false;
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + NAME_FIELD_NUMBER;
      hash = (53 * hash) + getName().hashCode();
      hash = (37 * hash) + DATA_TYPE_FIELD_NUMBER;
      hash = (53 * hash) + getDataType().hashCode();
      hash = (37 * hash) + REPEATED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(getRepeated());
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema parseFrom(
        java.nio.ByteBuffer data) throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }

    public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema parseFrom(
        java.nio.ByteBuffer data, com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }

    public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }

    public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }

    public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema parseFrom(
        byte[] data) throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }

    public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema parseFrom(
        byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }

    public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema parseFrom(
        java.io.InputStream input) throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3.parseWithIOException(PARSER, input);
    }

    public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema parseFrom(
        java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3.parseWithIOException(
          PARSER, input, extensionRegistry);
    }

    public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema
        parseDelimitedFrom(java.io.InputStream input) throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3.parseDelimitedWithIOException(PARSER, input);
    }

    public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema
        parseDelimitedFrom(
            java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3.parseDelimitedWithIOException(
          PARSER, input, extensionRegistry);
    }

    public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema parseFrom(
        com.google.protobuf.CodedInputStream input) throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3.parseWithIOException(PARSER, input);
    }

    public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3.parseWithIOException(
          PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() {
      return newBuilder();
    }

    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }

    public static Builder newBuilder(
        com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }

    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     *
     *
     * <pre>
     * Schema field definition.
     * </pre>
     *
     * Protobuf type {@code google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema}
     */
    public static final class Builder
        extends com.google.protobuf.GeneratedMessageV3.Builder<Builder>
        implements
        // @@protoc_insertion_point(builder_implements:google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema)
        com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor getDescriptor() {
        return com.google.cloud.aiplatform.v1beta1.ModelMonitorProto
            .internal_static_google_cloud_aiplatform_v1beta1_ModelMonitoringSchema_FieldSchema_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return com.google.cloud.aiplatform.v1beta1.ModelMonitorProto
            .internal_static_google_cloud_aiplatform_v1beta1_ModelMonitoringSchema_FieldSchema_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.class,
                com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder
                    .class);
      }

      // Construct using
      // com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.newBuilder()
      private Builder() {}

      private Builder(com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
      }

      @java.lang.Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        name_ = "";
        dataType_ = "";
        repeated_ = false;
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor getDescriptorForType() {
        return com.google.cloud.aiplatform.v1beta1.ModelMonitorProto
            .internal_static_google_cloud_aiplatform_v1beta1_ModelMonitoringSchema_FieldSchema_descriptor;
      }

      @java.lang.Override
      public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema
          getDefaultInstanceForType() {
        return com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema
            .getDefaultInstance();
      }

      @java.lang.Override
      public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema build() {
        com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema result =
            buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema buildPartial() {
        com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema result =
            new com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema(this);
        if (bitField0_ != 0) {
          buildPartial0(result);
        }
        onBuilt();
        return result;
      }

      private void buildPartial0(
          com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema result) {
        int from_bitField0_ = bitField0_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.name_ = name_;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.dataType_ = dataType_;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.repeated_ = repeated_;
        }
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }

      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field, java.lang.Object value) {
        return super.setField(field, value);
      }

      @java.lang.Override
      public Builder clearField(com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }

      @java.lang.Override
      public Builder clearOneof(com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }

      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index,
          java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }

      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field, java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }

      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other
            instanceof com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema) {
          return mergeFrom(
              (com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema) other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(
          com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema other) {
        if (other
            == com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema
                .getDefaultInstance()) return this;
        if (!other.getName().isEmpty()) {
          name_ = other.name_;
          bitField0_ |= 0x00000001;
          onChanged();
        }
        if (!other.getDataType().isEmpty()) {
          dataType_ = other.dataType_;
          bitField0_ |= 0x00000002;
          onChanged();
        }
        if (other.getRepeated() != false) {
          setRepeated(other.getRepeated());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10:
                {
                  name_ = input.readStringRequireUtf8();
                  bitField0_ |= 0x00000001;
                  break;
                } // case 10
              case 18:
                {
                  dataType_ = input.readStringRequireUtf8();
                  bitField0_ |= 0x00000002;
                  break;
                } // case 18
              case 24:
                {
                  repeated_ = input.readBool();
                  bitField0_ |= 0x00000004;
                  break;
                } // case 24
              default:
                {
                  if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                    done = true; // was an endgroup tag
                  }
                  break;
                } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }

      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       *
       *
       * <pre>
       * Field name.
       * </pre>
       *
       * <code>string name = 1;</code>
       *
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       *
       *
       * <pre>
       * Field name.
       * </pre>
       *
       * <code>string name = 1;</code>
       *
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b =
              com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       *
       *
       * <pre>
       * Field name.
       * </pre>
       *
       * <code>string name = 1;</code>
       *
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(java.lang.String value) {
        if (value == null) {
          throw new NullPointerException();
        }
        name_ = value;
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       *
       *
       * <pre>
       * Field name.
       * </pre>
       *
       * <code>string name = 1;</code>
       *
       * @return This builder for chaining.
       */
      public Builder clearName() {
        name_ = getDefaultInstance().getName();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }
      /**
       *
       *
       * <pre>
       * Field name.
       * </pre>
       *
       * <code>string name = 1;</code>
       *
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(com.google.protobuf.ByteString value) {
        if (value == null) {
          throw new NullPointerException();
        }
        checkByteStringIsUtf8(value);
        name_ = value;
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }

      private java.lang.Object dataType_ = "";
      /**
       *
       *
       * <pre>
       * Supported data types are:
       * `float`
       * `integer`
       * `boolean`
       * `string`
       * `categorical`
       * </pre>
       *
       * <code>string data_type = 2;</code>
       *
       * @return The dataType.
       */
      public java.lang.String getDataType() {
        java.lang.Object ref = dataType_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          dataType_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       *
       *
       * <pre>
       * Supported data types are:
       * `float`
       * `integer`
       * `boolean`
       * `string`
       * `categorical`
       * </pre>
       *
       * <code>string data_type = 2;</code>
       *
       * @return The bytes for dataType.
       */
      public com.google.protobuf.ByteString getDataTypeBytes() {
        java.lang.Object ref = dataType_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b =
              com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);
          dataType_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       *
       *
       * <pre>
       * Supported data types are:
       * `float`
       * `integer`
       * `boolean`
       * `string`
       * `categorical`
       * </pre>
       *
       * <code>string data_type = 2;</code>
       *
       * @param value The dataType to set.
       * @return This builder for chaining.
       */
      public Builder setDataType(java.lang.String value) {
        if (value == null) {
          throw new NullPointerException();
        }
        dataType_ = value;
        bitField0_ |= 0x00000002;
        onChanged();
        return this;
      }
      /**
       *
       *
       * <pre>
       * Supported data types are:
       * `float`
       * `integer`
       * `boolean`
       * `string`
       * `categorical`
       * </pre>
       *
       * <code>string data_type = 2;</code>
       *
       * @return This builder for chaining.
       */
      public Builder clearDataType() {
        dataType_ = getDefaultInstance().getDataType();
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
        return this;
      }
      /**
       *
       *
       * <pre>
       * Supported data types are:
       * `float`
       * `integer`
       * `boolean`
       * `string`
       * `categorical`
       * </pre>
       *
       * <code>string data_type = 2;</code>
       *
       * @param value The bytes for dataType to set.
       * @return This builder for chaining.
       */
      public Builder setDataTypeBytes(com.google.protobuf.ByteString value) {
        if (value == null) {
          throw new NullPointerException();
        }
        checkByteStringIsUtf8(value);
        dataType_ = value;
        bitField0_ |= 0x00000002;
        onChanged();
        return this;
      }

      private boolean repeated_;
      /**
       *
       *
       * <pre>
       * Describes if the schema field is an array of given data type.
       * </pre>
       *
       * <code>bool repeated = 3;</code>
       *
       * @return The repeated.
       */
      @java.lang.Override
      public boolean getRepeated() {
        return repeated_;
      }
      /**
       *
       *
       * <pre>
       * Describes if the schema field is an array of given data type.
       * </pre>
       *
       * <code>bool repeated = 3;</code>
       *
       * @param value The repeated to set.
       * @return This builder for chaining.
       */
      public Builder setRepeated(boolean value) {

        repeated_ = value;
        bitField0_ |= 0x00000004;
        onChanged();
        return this;
      }
      /**
       *
       *
       * <pre>
       * Describes if the schema field is an array of given data type.
       * </pre>
       *
       * <code>bool repeated = 3;</code>
       *
       * @return This builder for chaining.
       */
      public Builder clearRepeated() {
        bitField0_ = (bitField0_ & ~0x00000004);
        repeated_ = false;
        onChanged();
        return this;
      }

      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }

      // @@protoc_insertion_point(builder_scope:google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema)
    }

    // @@protoc_insertion_point(class_scope:google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema)
    private static final com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema
        DEFAULT_INSTANCE;

    static {
      DEFAULT_INSTANCE =
          new com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema();
    }

    public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema
        getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<FieldSchema> PARSER =
        new com.google.protobuf.AbstractParser<FieldSchema>() {
          @java.lang.Override
          public FieldSchema parsePartialFrom(
              com.google.protobuf.CodedInputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            Builder builder = newBuilder();
            try {
              builder.mergeFrom(input, extensionRegistry);
            } catch (com.google.protobuf.InvalidProtocolBufferException e) {
              throw e.setUnfinishedMessage(builder.buildPartial());
            } catch (com.google.protobuf.UninitializedMessageException e) {
              throw e.asInvalidProtocolBufferException()
                  .setUnfinishedMessage(builder.buildPartial());
            } catch (java.io.IOException e) {
              throw new com.google.protobuf.InvalidProtocolBufferException(e)
                  .setUnfinishedMessage(builder.buildPartial());
            }
            return builder.buildPartial();
          }
        };

    public static com.google.protobuf.Parser<FieldSchema> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<FieldSchema> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema
        getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }
  }

  public static final int FEATURE_FIELDS_FIELD_NUMBER = 1;

  @SuppressWarnings("serial")
  private java.util.List<com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema>
      featureFields_;
  /**
   *
   *
   * <pre>
   * Feature names of the model. Vertex AI will try to match the features from
   * your dataset as follows:
   *  * For 'csv' files, the header names are required, and we will extract the
   *    corresponding feature values when the header names align with the
   *    feature names.
   *  * For 'jsonl' files, we will extract the corresponding feature values if
   *    the key names match the feature names.
   *    Note: Nested features are not supported, so please ensure your features
   *    are flattened. Ensure the feature values are scalar or an array of
   *    scalars.
   *  * For 'bigquery' dataset, we will extract the corresponding feature values
   *    if the column names match the feature names.
   *    Note: The column type can be a scalar or an array of scalars. STRUCT or
   *    JSON types are not supported. You may use SQL queries to select or
   *    aggregate the relevant features from your original table. However,
   *    ensure that the 'schema' of the query results meets our requirements.
   *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
   *    Batch Prediction Job results. If the
   *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
   *    is an array, ensure that the sequence in
   *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
   *    matches the order of features in the prediction instance. We will match
   *    the feature with the array in the order specified in [feature_fields].
   * </pre>
   *
   * <code>
   * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
   * </code>
   */
  @java.lang.Override
  public java.util.List<com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema>
      getFeatureFieldsList() {
    return featureFields_;
  }
  /**
   *
   *
   * <pre>
   * Feature names of the model. Vertex AI will try to match the features from
   * your dataset as follows:
   *  * For 'csv' files, the header names are required, and we will extract the
   *    corresponding feature values when the header names align with the
   *    feature names.
   *  * For 'jsonl' files, we will extract the corresponding feature values if
   *    the key names match the feature names.
   *    Note: Nested features are not supported, so please ensure your features
   *    are flattened. Ensure the feature values are scalar or an array of
   *    scalars.
   *  * For 'bigquery' dataset, we will extract the corresponding feature values
   *    if the column names match the feature names.
   *    Note: The column type can be a scalar or an array of scalars. STRUCT or
   *    JSON types are not supported. You may use SQL queries to select or
   *    aggregate the relevant features from your original table. However,
   *    ensure that the 'schema' of the query results meets our requirements.
   *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
   *    Batch Prediction Job results. If the
   *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
   *    is an array, ensure that the sequence in
   *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
   *    matches the order of features in the prediction instance. We will match
   *    the feature with the array in the order specified in [feature_fields].
   * </pre>
   *
   * <code>
   * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
   * </code>
   */
  @java.lang.Override
  public java.util.List<
          ? extends com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder>
      getFeatureFieldsOrBuilderList() {
    return featureFields_;
  }
  /**
   *
   *
   * <pre>
   * Feature names of the model. Vertex AI will try to match the features from
   * your dataset as follows:
   *  * For 'csv' files, the header names are required, and we will extract the
   *    corresponding feature values when the header names align with the
   *    feature names.
   *  * For 'jsonl' files, we will extract the corresponding feature values if
   *    the key names match the feature names.
   *    Note: Nested features are not supported, so please ensure your features
   *    are flattened. Ensure the feature values are scalar or an array of
   *    scalars.
   *  * For 'bigquery' dataset, we will extract the corresponding feature values
   *    if the column names match the feature names.
   *    Note: The column type can be a scalar or an array of scalars. STRUCT or
   *    JSON types are not supported. You may use SQL queries to select or
   *    aggregate the relevant features from your original table. However,
   *    ensure that the 'schema' of the query results meets our requirements.
   *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
   *    Batch Prediction Job results. If the
   *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
   *    is an array, ensure that the sequence in
   *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
   *    matches the order of features in the prediction instance. We will match
   *    the feature with the array in the order specified in [feature_fields].
   * </pre>
   *
   * <code>
   * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
   * </code>
   */
  @java.lang.Override
  public int getFeatureFieldsCount() {
    return featureFields_.size();
  }
  /**
   *
   *
   * <pre>
   * Feature names of the model. Vertex AI will try to match the features from
   * your dataset as follows:
   *  * For 'csv' files, the header names are required, and we will extract the
   *    corresponding feature values when the header names align with the
   *    feature names.
   *  * For 'jsonl' files, we will extract the corresponding feature values if
   *    the key names match the feature names.
   *    Note: Nested features are not supported, so please ensure your features
   *    are flattened. Ensure the feature values are scalar or an array of
   *    scalars.
   *  * For 'bigquery' dataset, we will extract the corresponding feature values
   *    if the column names match the feature names.
   *    Note: The column type can be a scalar or an array of scalars. STRUCT or
   *    JSON types are not supported. You may use SQL queries to select or
   *    aggregate the relevant features from your original table. However,
   *    ensure that the 'schema' of the query results meets our requirements.
   *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
   *    Batch Prediction Job results. If the
   *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
   *    is an array, ensure that the sequence in
   *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
   *    matches the order of features in the prediction instance. We will match
   *    the feature with the array in the order specified in [feature_fields].
   * </pre>
   *
   * <code>
   * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
   * </code>
   */
  @java.lang.Override
  public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema getFeatureFields(
      int index) {
    return featureFields_.get(index);
  }
  /**
   *
   *
   * <pre>
   * Feature names of the model. Vertex AI will try to match the features from
   * your dataset as follows:
   *  * For 'csv' files, the header names are required, and we will extract the
   *    corresponding feature values when the header names align with the
   *    feature names.
   *  * For 'jsonl' files, we will extract the corresponding feature values if
   *    the key names match the feature names.
   *    Note: Nested features are not supported, so please ensure your features
   *    are flattened. Ensure the feature values are scalar or an array of
   *    scalars.
   *  * For 'bigquery' dataset, we will extract the corresponding feature values
   *    if the column names match the feature names.
   *    Note: The column type can be a scalar or an array of scalars. STRUCT or
   *    JSON types are not supported. You may use SQL queries to select or
   *    aggregate the relevant features from your original table. However,
   *    ensure that the 'schema' of the query results meets our requirements.
   *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
   *    Batch Prediction Job results. If the
   *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
   *    is an array, ensure that the sequence in
   *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
   *    matches the order of features in the prediction instance. We will match
   *    the feature with the array in the order specified in [feature_fields].
   * </pre>
   *
   * <code>
   * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
   * </code>
   */
  @java.lang.Override
  public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder
      getFeatureFieldsOrBuilder(int index) {
    return featureFields_.get(index);
  }

  public static final int PREDICTION_FIELDS_FIELD_NUMBER = 2;

  @SuppressWarnings("serial")
  private java.util.List<com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema>
      predictionFields_;
  /**
   *
   *
   * <pre>
   * Prediction output names of the model. The requirements are the same as the
   * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
   * For AutoML Tables, the prediction output name presented in schema will be:
   * `predicted_{target_column}`, the `target_column` is the one you specified
   * when you train the model.
   * For Prediction output drift analysis:
   *  * AutoML Classification, the distribution of the argmax label will be
   *    analyzed.
   *  * AutoML Regression, the distribution of the value will be analyzed.
   * </pre>
   *
   * <code>
   * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
   * </code>
   */
  @java.lang.Override
  public java.util.List<com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema>
      getPredictionFieldsList() {
    return predictionFields_;
  }
  /**
   *
   *
   * <pre>
   * Prediction output names of the model. The requirements are the same as the
   * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
   * For AutoML Tables, the prediction output name presented in schema will be:
   * `predicted_{target_column}`, the `target_column` is the one you specified
   * when you train the model.
   * For Prediction output drift analysis:
   *  * AutoML Classification, the distribution of the argmax label will be
   *    analyzed.
   *  * AutoML Regression, the distribution of the value will be analyzed.
   * </pre>
   *
   * <code>
   * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
   * </code>
   */
  @java.lang.Override
  public java.util.List<
          ? extends com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder>
      getPredictionFieldsOrBuilderList() {
    return predictionFields_;
  }
  /**
   *
   *
   * <pre>
   * Prediction output names of the model. The requirements are the same as the
   * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
   * For AutoML Tables, the prediction output name presented in schema will be:
   * `predicted_{target_column}`, the `target_column` is the one you specified
   * when you train the model.
   * For Prediction output drift analysis:
   *  * AutoML Classification, the distribution of the argmax label will be
   *    analyzed.
   *  * AutoML Regression, the distribution of the value will be analyzed.
   * </pre>
   *
   * <code>
   * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
   * </code>
   */
  @java.lang.Override
  public int getPredictionFieldsCount() {
    return predictionFields_.size();
  }
  /**
   *
   *
   * <pre>
   * Prediction output names of the model. The requirements are the same as the
   * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
   * For AutoML Tables, the prediction output name presented in schema will be:
   * `predicted_{target_column}`, the `target_column` is the one you specified
   * when you train the model.
   * For Prediction output drift analysis:
   *  * AutoML Classification, the distribution of the argmax label will be
   *    analyzed.
   *  * AutoML Regression, the distribution of the value will be analyzed.
   * </pre>
   *
   * <code>
   * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
   * </code>
   */
  @java.lang.Override
  public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema getPredictionFields(
      int index) {
    return predictionFields_.get(index);
  }
  /**
   *
   *
   * <pre>
   * Prediction output names of the model. The requirements are the same as the
   * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
   * For AutoML Tables, the prediction output name presented in schema will be:
   * `predicted_{target_column}`, the `target_column` is the one you specified
   * when you train the model.
   * For Prediction output drift analysis:
   *  * AutoML Classification, the distribution of the argmax label will be
   *    analyzed.
   *  * AutoML Regression, the distribution of the value will be analyzed.
   * </pre>
   *
   * <code>
   * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
   * </code>
   */
  @java.lang.Override
  public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder
      getPredictionFieldsOrBuilder(int index) {
    return predictionFields_.get(index);
  }

  public static final int GROUND_TRUTH_FIELDS_FIELD_NUMBER = 3;

  @SuppressWarnings("serial")
  private java.util.List<com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema>
      groundTruthFields_;
  /**
   *
   *
   * <pre>
   * Target /ground truth names of the model.
   * </pre>
   *
   * <code>
   * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
   * </code>
   */
  @java.lang.Override
  public java.util.List<com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema>
      getGroundTruthFieldsList() {
    return groundTruthFields_;
  }
  /**
   *
   *
   * <pre>
   * Target /ground truth names of the model.
   * </pre>
   *
   * <code>
   * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
   * </code>
   */
  @java.lang.Override
  public java.util.List<
          ? extends com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder>
      getGroundTruthFieldsOrBuilderList() {
    return groundTruthFields_;
  }
  /**
   *
   *
   * <pre>
   * Target /ground truth names of the model.
   * </pre>
   *
   * <code>
   * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
   * </code>
   */
  @java.lang.Override
  public int getGroundTruthFieldsCount() {
    return groundTruthFields_.size();
  }
  /**
   *
   *
   * <pre>
   * Target /ground truth names of the model.
   * </pre>
   *
   * <code>
   * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
   * </code>
   */
  @java.lang.Override
  public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema getGroundTruthFields(
      int index) {
    return groundTruthFields_.get(index);
  }
  /**
   *
   *
   * <pre>
   * Target /ground truth names of the model.
   * </pre>
   *
   * <code>
   * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
   * </code>
   */
  @java.lang.Override
  public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder
      getGroundTruthFieldsOrBuilder(int index) {
    return groundTruthFields_.get(index);
  }

  private byte memoizedIsInitialized = -1;

  @java.lang.Override
  public final boolean isInitialized() {
    byte isInitialized = memoizedIsInitialized;
    if (isInitialized == 1) return true;
    if (isInitialized == 0) return false;

    memoizedIsInitialized = 1;
    return true;
  }

  @java.lang.Override
  public void writeTo(com.google.protobuf.CodedOutputStream output) throws java.io.IOException {
    for (int i = 0; i < featureFields_.size(); i++) {
      output.writeMessage(1, featureFields_.get(i));
    }
    for (int i = 0; i < predictionFields_.size(); i++) {
      output.writeMessage(2, predictionFields_.get(i));
    }
    for (int i = 0; i < groundTruthFields_.size(); i++) {
      output.writeMessage(3, groundTruthFields_.get(i));
    }
    getUnknownFields().writeTo(output);
  }

  @java.lang.Override
  public int getSerializedSize() {
    int size = memoizedSize;
    if (size != -1) return size;

    size = 0;
    for (int i = 0; i < featureFields_.size(); i++) {
      size += com.google.protobuf.CodedOutputStream.computeMessageSize(1, featureFields_.get(i));
    }
    for (int i = 0; i < predictionFields_.size(); i++) {
      size += com.google.protobuf.CodedOutputStream.computeMessageSize(2, predictionFields_.get(i));
    }
    for (int i = 0; i < groundTruthFields_.size(); i++) {
      size +=
          com.google.protobuf.CodedOutputStream.computeMessageSize(3, groundTruthFields_.get(i));
    }
    size += getUnknownFields().getSerializedSize();
    memoizedSize = size;
    return size;
  }

  @java.lang.Override
  public boolean equals(final java.lang.Object obj) {
    if (obj == this) {
      return true;
    }
    if (!(obj instanceof com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema)) {
      return super.equals(obj);
    }
    com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema other =
        (com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema) obj;

    if (!getFeatureFieldsList().equals(other.getFeatureFieldsList())) return false;
    if (!getPredictionFieldsList().equals(other.getPredictionFieldsList())) return false;
    if (!getGroundTruthFieldsList().equals(other.getGroundTruthFieldsList())) return false;
    if (!getUnknownFields().equals(other.getUnknownFields())) return false;
    return true;
  }

  @java.lang.Override
  public int hashCode() {
    if (memoizedHashCode != 0) {
      return memoizedHashCode;
    }
    int hash = 41;
    hash = (19 * hash) + getDescriptor().hashCode();
    if (getFeatureFieldsCount() > 0) {
      hash = (37 * hash) + FEATURE_FIELDS_FIELD_NUMBER;
      hash = (53 * hash) + getFeatureFieldsList().hashCode();
    }
    if (getPredictionFieldsCount() > 0) {
      hash = (37 * hash) + PREDICTION_FIELDS_FIELD_NUMBER;
      hash = (53 * hash) + getPredictionFieldsList().hashCode();
    }
    if (getGroundTruthFieldsCount() > 0) {
      hash = (37 * hash) + GROUND_TRUTH_FIELDS_FIELD_NUMBER;
      hash = (53 * hash) + getGroundTruthFieldsList().hashCode();
    }
    hash = (29 * hash) + getUnknownFields().hashCode();
    memoizedHashCode = hash;
    return hash;
  }

  public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema parseFrom(
      java.nio.ByteBuffer data) throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }

  public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema parseFrom(
      java.nio.ByteBuffer data, com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }

  public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema parseFrom(
      com.google.protobuf.ByteString data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }

  public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema parseFrom(
      com.google.protobuf.ByteString data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }

  public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema parseFrom(byte[] data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }

  public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema parseFrom(
      byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }

  public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema parseFrom(
      java.io.InputStream input) throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3.parseWithIOException(PARSER, input);
  }

  public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema parseFrom(
      java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3.parseWithIOException(
        PARSER, input, extensionRegistry);
  }

  public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema parseDelimitedFrom(
      java.io.InputStream input) throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3.parseDelimitedWithIOException(PARSER, input);
  }

  public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema parseDelimitedFrom(
      java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3.parseDelimitedWithIOException(
        PARSER, input, extensionRegistry);
  }

  public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema parseFrom(
      com.google.protobuf.CodedInputStream input) throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3.parseWithIOException(PARSER, input);
  }

  public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema parseFrom(
      com.google.protobuf.CodedInputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3.parseWithIOException(
        PARSER, input, extensionRegistry);
  }

  @java.lang.Override
  public Builder newBuilderForType() {
    return newBuilder();
  }

  public static Builder newBuilder() {
    return DEFAULT_INSTANCE.toBuilder();
  }

  public static Builder newBuilder(
      com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema prototype) {
    return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
  }

  @java.lang.Override
  public Builder toBuilder() {
    return this == DEFAULT_INSTANCE ? new Builder() : new Builder().mergeFrom(this);
  }

  @java.lang.Override
  protected Builder newBuilderForType(com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
    Builder builder = new Builder(parent);
    return builder;
  }
  /**
   *
   *
   * <pre>
   * The Model Monitoring Schema definition.
   * </pre>
   *
   * Protobuf type {@code google.cloud.aiplatform.v1beta1.ModelMonitoringSchema}
   */
  public static final class Builder extends com.google.protobuf.GeneratedMessageV3.Builder<Builder>
      implements
      // @@protoc_insertion_point(builder_implements:google.cloud.aiplatform.v1beta1.ModelMonitoringSchema)
      com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchemaOrBuilder {
    public static final com.google.protobuf.Descriptors.Descriptor getDescriptor() {
      return com.google.cloud.aiplatform.v1beta1.ModelMonitorProto
          .internal_static_google_cloud_aiplatform_v1beta1_ModelMonitoringSchema_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return com.google.cloud.aiplatform.v1beta1.ModelMonitorProto
          .internal_static_google_cloud_aiplatform_v1beta1_ModelMonitoringSchema_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.class,
              com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.Builder.class);
    }

    // Construct using com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.newBuilder()
    private Builder() {}

    private Builder(com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      super(parent);
    }

    @java.lang.Override
    public Builder clear() {
      super.clear();
      bitField0_ = 0;
      if (featureFieldsBuilder_ == null) {
        featureFields_ = java.util.Collections.emptyList();
      } else {
        featureFields_ = null;
        featureFieldsBuilder_.clear();
      }
      bitField0_ = (bitField0_ & ~0x00000001);
      if (predictionFieldsBuilder_ == null) {
        predictionFields_ = java.util.Collections.emptyList();
      } else {
        predictionFields_ = null;
        predictionFieldsBuilder_.clear();
      }
      bitField0_ = (bitField0_ & ~0x00000002);
      if (groundTruthFieldsBuilder_ == null) {
        groundTruthFields_ = java.util.Collections.emptyList();
      } else {
        groundTruthFields_ = null;
        groundTruthFieldsBuilder_.clear();
      }
      bitField0_ = (bitField0_ & ~0x00000004);
      return this;
    }

    @java.lang.Override
    public com.google.protobuf.Descriptors.Descriptor getDescriptorForType() {
      return com.google.cloud.aiplatform.v1beta1.ModelMonitorProto
          .internal_static_google_cloud_aiplatform_v1beta1_ModelMonitoringSchema_descriptor;
    }

    @java.lang.Override
    public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema getDefaultInstanceForType() {
      return com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.getDefaultInstance();
    }

    @java.lang.Override
    public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema build() {
      com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema result = buildPartial();
      if (!result.isInitialized()) {
        throw newUninitializedMessageException(result);
      }
      return result;
    }

    @java.lang.Override
    public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema buildPartial() {
      com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema result =
          new com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema(this);
      buildPartialRepeatedFields(result);
      if (bitField0_ != 0) {
        buildPartial0(result);
      }
      onBuilt();
      return result;
    }

    private void buildPartialRepeatedFields(
        com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema result) {
      if (featureFieldsBuilder_ == null) {
        if (((bitField0_ & 0x00000001) != 0)) {
          featureFields_ = java.util.Collections.unmodifiableList(featureFields_);
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.featureFields_ = featureFields_;
      } else {
        result.featureFields_ = featureFieldsBuilder_.build();
      }
      if (predictionFieldsBuilder_ == null) {
        if (((bitField0_ & 0x00000002) != 0)) {
          predictionFields_ = java.util.Collections.unmodifiableList(predictionFields_);
          bitField0_ = (bitField0_ & ~0x00000002);
        }
        result.predictionFields_ = predictionFields_;
      } else {
        result.predictionFields_ = predictionFieldsBuilder_.build();
      }
      if (groundTruthFieldsBuilder_ == null) {
        if (((bitField0_ & 0x00000004) != 0)) {
          groundTruthFields_ = java.util.Collections.unmodifiableList(groundTruthFields_);
          bitField0_ = (bitField0_ & ~0x00000004);
        }
        result.groundTruthFields_ = groundTruthFields_;
      } else {
        result.groundTruthFields_ = groundTruthFieldsBuilder_.build();
      }
    }

    private void buildPartial0(com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema result) {
      int from_bitField0_ = bitField0_;
    }

    @java.lang.Override
    public Builder clone() {
      return super.clone();
    }

    @java.lang.Override
    public Builder setField(
        com.google.protobuf.Descriptors.FieldDescriptor field, java.lang.Object value) {
      return super.setField(field, value);
    }

    @java.lang.Override
    public Builder clearField(com.google.protobuf.Descriptors.FieldDescriptor field) {
      return super.clearField(field);
    }

    @java.lang.Override
    public Builder clearOneof(com.google.protobuf.Descriptors.OneofDescriptor oneof) {
      return super.clearOneof(oneof);
    }

    @java.lang.Override
    public Builder setRepeatedField(
        com.google.protobuf.Descriptors.FieldDescriptor field, int index, java.lang.Object value) {
      return super.setRepeatedField(field, index, value);
    }

    @java.lang.Override
    public Builder addRepeatedField(
        com.google.protobuf.Descriptors.FieldDescriptor field, java.lang.Object value) {
      return super.addRepeatedField(field, value);
    }

    @java.lang.Override
    public Builder mergeFrom(com.google.protobuf.Message other) {
      if (other instanceof com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema) {
        return mergeFrom((com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema) other);
      } else {
        super.mergeFrom(other);
        return this;
      }
    }

    public Builder mergeFrom(com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema other) {
      if (other == com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.getDefaultInstance())
        return this;
      if (featureFieldsBuilder_ == null) {
        if (!other.featureFields_.isEmpty()) {
          if (featureFields_.isEmpty()) {
            featureFields_ = other.featureFields_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureFeatureFieldsIsMutable();
            featureFields_.addAll(other.featureFields_);
          }
          onChanged();
        }
      } else {
        if (!other.featureFields_.isEmpty()) {
          if (featureFieldsBuilder_.isEmpty()) {
            featureFieldsBuilder_.dispose();
            featureFieldsBuilder_ = null;
            featureFields_ = other.featureFields_;
            bitField0_ = (bitField0_ & ~0x00000001);
            featureFieldsBuilder_ =
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders
                    ? getFeatureFieldsFieldBuilder()
                    : null;
          } else {
            featureFieldsBuilder_.addAllMessages(other.featureFields_);
          }
        }
      }
      if (predictionFieldsBuilder_ == null) {
        if (!other.predictionFields_.isEmpty()) {
          if (predictionFields_.isEmpty()) {
            predictionFields_ = other.predictionFields_;
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            ensurePredictionFieldsIsMutable();
            predictionFields_.addAll(other.predictionFields_);
          }
          onChanged();
        }
      } else {
        if (!other.predictionFields_.isEmpty()) {
          if (predictionFieldsBuilder_.isEmpty()) {
            predictionFieldsBuilder_.dispose();
            predictionFieldsBuilder_ = null;
            predictionFields_ = other.predictionFields_;
            bitField0_ = (bitField0_ & ~0x00000002);
            predictionFieldsBuilder_ =
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders
                    ? getPredictionFieldsFieldBuilder()
                    : null;
          } else {
            predictionFieldsBuilder_.addAllMessages(other.predictionFields_);
          }
        }
      }
      if (groundTruthFieldsBuilder_ == null) {
        if (!other.groundTruthFields_.isEmpty()) {
          if (groundTruthFields_.isEmpty()) {
            groundTruthFields_ = other.groundTruthFields_;
            bitField0_ = (bitField0_ & ~0x00000004);
          } else {
            ensureGroundTruthFieldsIsMutable();
            groundTruthFields_.addAll(other.groundTruthFields_);
          }
          onChanged();
        }
      } else {
        if (!other.groundTruthFields_.isEmpty()) {
          if (groundTruthFieldsBuilder_.isEmpty()) {
            groundTruthFieldsBuilder_.dispose();
            groundTruthFieldsBuilder_ = null;
            groundTruthFields_ = other.groundTruthFields_;
            bitField0_ = (bitField0_ & ~0x00000004);
            groundTruthFieldsBuilder_ =
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders
                    ? getGroundTruthFieldsFieldBuilder()
                    : null;
          } else {
            groundTruthFieldsBuilder_.addAllMessages(other.groundTruthFields_);
          }
        }
      }
      this.mergeUnknownFields(other.getUnknownFields());
      onChanged();
      return this;
    }

    @java.lang.Override
    public final boolean isInitialized() {
      return true;
    }

    @java.lang.Override
    public Builder mergeFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10:
              {
                com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema m =
                    input.readMessage(
                        com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema
                            .parser(),
                        extensionRegistry);
                if (featureFieldsBuilder_ == null) {
                  ensureFeatureFieldsIsMutable();
                  featureFields_.add(m);
                } else {
                  featureFieldsBuilder_.addMessage(m);
                }
                break;
              } // case 10
            case 18:
              {
                com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema m =
                    input.readMessage(
                        com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema
                            .parser(),
                        extensionRegistry);
                if (predictionFieldsBuilder_ == null) {
                  ensurePredictionFieldsIsMutable();
                  predictionFields_.add(m);
                } else {
                  predictionFieldsBuilder_.addMessage(m);
                }
                break;
              } // case 18
            case 26:
              {
                com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema m =
                    input.readMessage(
                        com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema
                            .parser(),
                        extensionRegistry);
                if (groundTruthFieldsBuilder_ == null) {
                  ensureGroundTruthFieldsIsMutable();
                  groundTruthFields_.add(m);
                } else {
                  groundTruthFieldsBuilder_.addMessage(m);
                }
                break;
              } // case 26
            default:
              {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
          } // switch (tag)
        } // while (!done)
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.unwrapIOException();
      } finally {
        onChanged();
      } // finally
      return this;
    }

    private int bitField0_;

    private java.util.List<com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema>
        featureFields_ = java.util.Collections.emptyList();

    private void ensureFeatureFieldsIsMutable() {
      if (!((bitField0_ & 0x00000001) != 0)) {
        featureFields_ =
            new java.util.ArrayList<
                com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema>(
                featureFields_);
        bitField0_ |= 0x00000001;
      }
    }

    private com.google.protobuf.RepeatedFieldBuilderV3<
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema,
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder,
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder>
        featureFieldsBuilder_;

    /**
     *
     *
     * <pre>
     * Feature names of the model. Vertex AI will try to match the features from
     * your dataset as follows:
     *  * For 'csv' files, the header names are required, and we will extract the
     *    corresponding feature values when the header names align with the
     *    feature names.
     *  * For 'jsonl' files, we will extract the corresponding feature values if
     *    the key names match the feature names.
     *    Note: Nested features are not supported, so please ensure your features
     *    are flattened. Ensure the feature values are scalar or an array of
     *    scalars.
     *  * For 'bigquery' dataset, we will extract the corresponding feature values
     *    if the column names match the feature names.
     *    Note: The column type can be a scalar or an array of scalars. STRUCT or
     *    JSON types are not supported. You may use SQL queries to select or
     *    aggregate the relevant features from your original table. However,
     *    ensure that the 'schema' of the query results meets our requirements.
     *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
     *    Batch Prediction Job results. If the
     *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
     *    is an array, ensure that the sequence in
     *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
     *    matches the order of features in the prediction instance. We will match
     *    the feature with the array in the order specified in [feature_fields].
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
     * </code>
     */
    public java.util.List<com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema>
        getFeatureFieldsList() {
      if (featureFieldsBuilder_ == null) {
        return java.util.Collections.unmodifiableList(featureFields_);
      } else {
        return featureFieldsBuilder_.getMessageList();
      }
    }
    /**
     *
     *
     * <pre>
     * Feature names of the model. Vertex AI will try to match the features from
     * your dataset as follows:
     *  * For 'csv' files, the header names are required, and we will extract the
     *    corresponding feature values when the header names align with the
     *    feature names.
     *  * For 'jsonl' files, we will extract the corresponding feature values if
     *    the key names match the feature names.
     *    Note: Nested features are not supported, so please ensure your features
     *    are flattened. Ensure the feature values are scalar or an array of
     *    scalars.
     *  * For 'bigquery' dataset, we will extract the corresponding feature values
     *    if the column names match the feature names.
     *    Note: The column type can be a scalar or an array of scalars. STRUCT or
     *    JSON types are not supported. You may use SQL queries to select or
     *    aggregate the relevant features from your original table. However,
     *    ensure that the 'schema' of the query results meets our requirements.
     *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
     *    Batch Prediction Job results. If the
     *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
     *    is an array, ensure that the sequence in
     *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
     *    matches the order of features in the prediction instance. We will match
     *    the feature with the array in the order specified in [feature_fields].
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
     * </code>
     */
    public int getFeatureFieldsCount() {
      if (featureFieldsBuilder_ == null) {
        return featureFields_.size();
      } else {
        return featureFieldsBuilder_.getCount();
      }
    }
    /**
     *
     *
     * <pre>
     * Feature names of the model. Vertex AI will try to match the features from
     * your dataset as follows:
     *  * For 'csv' files, the header names are required, and we will extract the
     *    corresponding feature values when the header names align with the
     *    feature names.
     *  * For 'jsonl' files, we will extract the corresponding feature values if
     *    the key names match the feature names.
     *    Note: Nested features are not supported, so please ensure your features
     *    are flattened. Ensure the feature values are scalar or an array of
     *    scalars.
     *  * For 'bigquery' dataset, we will extract the corresponding feature values
     *    if the column names match the feature names.
     *    Note: The column type can be a scalar or an array of scalars. STRUCT or
     *    JSON types are not supported. You may use SQL queries to select or
     *    aggregate the relevant features from your original table. However,
     *    ensure that the 'schema' of the query results meets our requirements.
     *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
     *    Batch Prediction Job results. If the
     *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
     *    is an array, ensure that the sequence in
     *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
     *    matches the order of features in the prediction instance. We will match
     *    the feature with the array in the order specified in [feature_fields].
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
     * </code>
     */
    public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema getFeatureFields(
        int index) {
      if (featureFieldsBuilder_ == null) {
        return featureFields_.get(index);
      } else {
        return featureFieldsBuilder_.getMessage(index);
      }
    }
    /**
     *
     *
     * <pre>
     * Feature names of the model. Vertex AI will try to match the features from
     * your dataset as follows:
     *  * For 'csv' files, the header names are required, and we will extract the
     *    corresponding feature values when the header names align with the
     *    feature names.
     *  * For 'jsonl' files, we will extract the corresponding feature values if
     *    the key names match the feature names.
     *    Note: Nested features are not supported, so please ensure your features
     *    are flattened. Ensure the feature values are scalar or an array of
     *    scalars.
     *  * For 'bigquery' dataset, we will extract the corresponding feature values
     *    if the column names match the feature names.
     *    Note: The column type can be a scalar or an array of scalars. STRUCT or
     *    JSON types are not supported. You may use SQL queries to select or
     *    aggregate the relevant features from your original table. However,
     *    ensure that the 'schema' of the query results meets our requirements.
     *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
     *    Batch Prediction Job results. If the
     *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
     *    is an array, ensure that the sequence in
     *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
     *    matches the order of features in the prediction instance. We will match
     *    the feature with the array in the order specified in [feature_fields].
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
     * </code>
     */
    public Builder setFeatureFields(
        int index, com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema value) {
      if (featureFieldsBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureFeatureFieldsIsMutable();
        featureFields_.set(index, value);
        onChanged();
      } else {
        featureFieldsBuilder_.setMessage(index, value);
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Feature names of the model. Vertex AI will try to match the features from
     * your dataset as follows:
     *  * For 'csv' files, the header names are required, and we will extract the
     *    corresponding feature values when the header names align with the
     *    feature names.
     *  * For 'jsonl' files, we will extract the corresponding feature values if
     *    the key names match the feature names.
     *    Note: Nested features are not supported, so please ensure your features
     *    are flattened. Ensure the feature values are scalar or an array of
     *    scalars.
     *  * For 'bigquery' dataset, we will extract the corresponding feature values
     *    if the column names match the feature names.
     *    Note: The column type can be a scalar or an array of scalars. STRUCT or
     *    JSON types are not supported. You may use SQL queries to select or
     *    aggregate the relevant features from your original table. However,
     *    ensure that the 'schema' of the query results meets our requirements.
     *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
     *    Batch Prediction Job results. If the
     *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
     *    is an array, ensure that the sequence in
     *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
     *    matches the order of features in the prediction instance. We will match
     *    the feature with the array in the order specified in [feature_fields].
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
     * </code>
     */
    public Builder setFeatureFields(
        int index,
        com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder
            builderForValue) {
      if (featureFieldsBuilder_ == null) {
        ensureFeatureFieldsIsMutable();
        featureFields_.set(index, builderForValue.build());
        onChanged();
      } else {
        featureFieldsBuilder_.setMessage(index, builderForValue.build());
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Feature names of the model. Vertex AI will try to match the features from
     * your dataset as follows:
     *  * For 'csv' files, the header names are required, and we will extract the
     *    corresponding feature values when the header names align with the
     *    feature names.
     *  * For 'jsonl' files, we will extract the corresponding feature values if
     *    the key names match the feature names.
     *    Note: Nested features are not supported, so please ensure your features
     *    are flattened. Ensure the feature values are scalar or an array of
     *    scalars.
     *  * For 'bigquery' dataset, we will extract the corresponding feature values
     *    if the column names match the feature names.
     *    Note: The column type can be a scalar or an array of scalars. STRUCT or
     *    JSON types are not supported. You may use SQL queries to select or
     *    aggregate the relevant features from your original table. However,
     *    ensure that the 'schema' of the query results meets our requirements.
     *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
     *    Batch Prediction Job results. If the
     *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
     *    is an array, ensure that the sequence in
     *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
     *    matches the order of features in the prediction instance. We will match
     *    the feature with the array in the order specified in [feature_fields].
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
     * </code>
     */
    public Builder addFeatureFields(
        com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema value) {
      if (featureFieldsBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureFeatureFieldsIsMutable();
        featureFields_.add(value);
        onChanged();
      } else {
        featureFieldsBuilder_.addMessage(value);
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Feature names of the model. Vertex AI will try to match the features from
     * your dataset as follows:
     *  * For 'csv' files, the header names are required, and we will extract the
     *    corresponding feature values when the header names align with the
     *    feature names.
     *  * For 'jsonl' files, we will extract the corresponding feature values if
     *    the key names match the feature names.
     *    Note: Nested features are not supported, so please ensure your features
     *    are flattened. Ensure the feature values are scalar or an array of
     *    scalars.
     *  * For 'bigquery' dataset, we will extract the corresponding feature values
     *    if the column names match the feature names.
     *    Note: The column type can be a scalar or an array of scalars. STRUCT or
     *    JSON types are not supported. You may use SQL queries to select or
     *    aggregate the relevant features from your original table. However,
     *    ensure that the 'schema' of the query results meets our requirements.
     *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
     *    Batch Prediction Job results. If the
     *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
     *    is an array, ensure that the sequence in
     *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
     *    matches the order of features in the prediction instance. We will match
     *    the feature with the array in the order specified in [feature_fields].
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
     * </code>
     */
    public Builder addFeatureFields(
        int index, com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema value) {
      if (featureFieldsBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureFeatureFieldsIsMutable();
        featureFields_.add(index, value);
        onChanged();
      } else {
        featureFieldsBuilder_.addMessage(index, value);
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Feature names of the model. Vertex AI will try to match the features from
     * your dataset as follows:
     *  * For 'csv' files, the header names are required, and we will extract the
     *    corresponding feature values when the header names align with the
     *    feature names.
     *  * For 'jsonl' files, we will extract the corresponding feature values if
     *    the key names match the feature names.
     *    Note: Nested features are not supported, so please ensure your features
     *    are flattened. Ensure the feature values are scalar or an array of
     *    scalars.
     *  * For 'bigquery' dataset, we will extract the corresponding feature values
     *    if the column names match the feature names.
     *    Note: The column type can be a scalar or an array of scalars. STRUCT or
     *    JSON types are not supported. You may use SQL queries to select or
     *    aggregate the relevant features from your original table. However,
     *    ensure that the 'schema' of the query results meets our requirements.
     *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
     *    Batch Prediction Job results. If the
     *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
     *    is an array, ensure that the sequence in
     *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
     *    matches the order of features in the prediction instance. We will match
     *    the feature with the array in the order specified in [feature_fields].
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
     * </code>
     */
    public Builder addFeatureFields(
        com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder
            builderForValue) {
      if (featureFieldsBuilder_ == null) {
        ensureFeatureFieldsIsMutable();
        featureFields_.add(builderForValue.build());
        onChanged();
      } else {
        featureFieldsBuilder_.addMessage(builderForValue.build());
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Feature names of the model. Vertex AI will try to match the features from
     * your dataset as follows:
     *  * For 'csv' files, the header names are required, and we will extract the
     *    corresponding feature values when the header names align with the
     *    feature names.
     *  * For 'jsonl' files, we will extract the corresponding feature values if
     *    the key names match the feature names.
     *    Note: Nested features are not supported, so please ensure your features
     *    are flattened. Ensure the feature values are scalar or an array of
     *    scalars.
     *  * For 'bigquery' dataset, we will extract the corresponding feature values
     *    if the column names match the feature names.
     *    Note: The column type can be a scalar or an array of scalars. STRUCT or
     *    JSON types are not supported. You may use SQL queries to select or
     *    aggregate the relevant features from your original table. However,
     *    ensure that the 'schema' of the query results meets our requirements.
     *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
     *    Batch Prediction Job results. If the
     *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
     *    is an array, ensure that the sequence in
     *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
     *    matches the order of features in the prediction instance. We will match
     *    the feature with the array in the order specified in [feature_fields].
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
     * </code>
     */
    public Builder addFeatureFields(
        int index,
        com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder
            builderForValue) {
      if (featureFieldsBuilder_ == null) {
        ensureFeatureFieldsIsMutable();
        featureFields_.add(index, builderForValue.build());
        onChanged();
      } else {
        featureFieldsBuilder_.addMessage(index, builderForValue.build());
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Feature names of the model. Vertex AI will try to match the features from
     * your dataset as follows:
     *  * For 'csv' files, the header names are required, and we will extract the
     *    corresponding feature values when the header names align with the
     *    feature names.
     *  * For 'jsonl' files, we will extract the corresponding feature values if
     *    the key names match the feature names.
     *    Note: Nested features are not supported, so please ensure your features
     *    are flattened. Ensure the feature values are scalar or an array of
     *    scalars.
     *  * For 'bigquery' dataset, we will extract the corresponding feature values
     *    if the column names match the feature names.
     *    Note: The column type can be a scalar or an array of scalars. STRUCT or
     *    JSON types are not supported. You may use SQL queries to select or
     *    aggregate the relevant features from your original table. However,
     *    ensure that the 'schema' of the query results meets our requirements.
     *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
     *    Batch Prediction Job results. If the
     *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
     *    is an array, ensure that the sequence in
     *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
     *    matches the order of features in the prediction instance. We will match
     *    the feature with the array in the order specified in [feature_fields].
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
     * </code>
     */
    public Builder addAllFeatureFields(
        java.lang.Iterable<
                ? extends com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema>
            values) {
      if (featureFieldsBuilder_ == null) {
        ensureFeatureFieldsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(values, featureFields_);
        onChanged();
      } else {
        featureFieldsBuilder_.addAllMessages(values);
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Feature names of the model. Vertex AI will try to match the features from
     * your dataset as follows:
     *  * For 'csv' files, the header names are required, and we will extract the
     *    corresponding feature values when the header names align with the
     *    feature names.
     *  * For 'jsonl' files, we will extract the corresponding feature values if
     *    the key names match the feature names.
     *    Note: Nested features are not supported, so please ensure your features
     *    are flattened. Ensure the feature values are scalar or an array of
     *    scalars.
     *  * For 'bigquery' dataset, we will extract the corresponding feature values
     *    if the column names match the feature names.
     *    Note: The column type can be a scalar or an array of scalars. STRUCT or
     *    JSON types are not supported. You may use SQL queries to select or
     *    aggregate the relevant features from your original table. However,
     *    ensure that the 'schema' of the query results meets our requirements.
     *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
     *    Batch Prediction Job results. If the
     *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
     *    is an array, ensure that the sequence in
     *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
     *    matches the order of features in the prediction instance. We will match
     *    the feature with the array in the order specified in [feature_fields].
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
     * </code>
     */
    public Builder clearFeatureFields() {
      if (featureFieldsBuilder_ == null) {
        featureFields_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
      } else {
        featureFieldsBuilder_.clear();
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Feature names of the model. Vertex AI will try to match the features from
     * your dataset as follows:
     *  * For 'csv' files, the header names are required, and we will extract the
     *    corresponding feature values when the header names align with the
     *    feature names.
     *  * For 'jsonl' files, we will extract the corresponding feature values if
     *    the key names match the feature names.
     *    Note: Nested features are not supported, so please ensure your features
     *    are flattened. Ensure the feature values are scalar or an array of
     *    scalars.
     *  * For 'bigquery' dataset, we will extract the corresponding feature values
     *    if the column names match the feature names.
     *    Note: The column type can be a scalar or an array of scalars. STRUCT or
     *    JSON types are not supported. You may use SQL queries to select or
     *    aggregate the relevant features from your original table. However,
     *    ensure that the 'schema' of the query results meets our requirements.
     *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
     *    Batch Prediction Job results. If the
     *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
     *    is an array, ensure that the sequence in
     *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
     *    matches the order of features in the prediction instance. We will match
     *    the feature with the array in the order specified in [feature_fields].
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
     * </code>
     */
    public Builder removeFeatureFields(int index) {
      if (featureFieldsBuilder_ == null) {
        ensureFeatureFieldsIsMutable();
        featureFields_.remove(index);
        onChanged();
      } else {
        featureFieldsBuilder_.remove(index);
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Feature names of the model. Vertex AI will try to match the features from
     * your dataset as follows:
     *  * For 'csv' files, the header names are required, and we will extract the
     *    corresponding feature values when the header names align with the
     *    feature names.
     *  * For 'jsonl' files, we will extract the corresponding feature values if
     *    the key names match the feature names.
     *    Note: Nested features are not supported, so please ensure your features
     *    are flattened. Ensure the feature values are scalar or an array of
     *    scalars.
     *  * For 'bigquery' dataset, we will extract the corresponding feature values
     *    if the column names match the feature names.
     *    Note: The column type can be a scalar or an array of scalars. STRUCT or
     *    JSON types are not supported. You may use SQL queries to select or
     *    aggregate the relevant features from your original table. However,
     *    ensure that the 'schema' of the query results meets our requirements.
     *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
     *    Batch Prediction Job results. If the
     *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
     *    is an array, ensure that the sequence in
     *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
     *    matches the order of features in the prediction instance. We will match
     *    the feature with the array in the order specified in [feature_fields].
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
     * </code>
     */
    public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder
        getFeatureFieldsBuilder(int index) {
      return getFeatureFieldsFieldBuilder().getBuilder(index);
    }
    /**
     *
     *
     * <pre>
     * Feature names of the model. Vertex AI will try to match the features from
     * your dataset as follows:
     *  * For 'csv' files, the header names are required, and we will extract the
     *    corresponding feature values when the header names align with the
     *    feature names.
     *  * For 'jsonl' files, we will extract the corresponding feature values if
     *    the key names match the feature names.
     *    Note: Nested features are not supported, so please ensure your features
     *    are flattened. Ensure the feature values are scalar or an array of
     *    scalars.
     *  * For 'bigquery' dataset, we will extract the corresponding feature values
     *    if the column names match the feature names.
     *    Note: The column type can be a scalar or an array of scalars. STRUCT or
     *    JSON types are not supported. You may use SQL queries to select or
     *    aggregate the relevant features from your original table. However,
     *    ensure that the 'schema' of the query results meets our requirements.
     *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
     *    Batch Prediction Job results. If the
     *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
     *    is an array, ensure that the sequence in
     *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
     *    matches the order of features in the prediction instance. We will match
     *    the feature with the array in the order specified in [feature_fields].
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
     * </code>
     */
    public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder
        getFeatureFieldsOrBuilder(int index) {
      if (featureFieldsBuilder_ == null) {
        return featureFields_.get(index);
      } else {
        return featureFieldsBuilder_.getMessageOrBuilder(index);
      }
    }
    /**
     *
     *
     * <pre>
     * Feature names of the model. Vertex AI will try to match the features from
     * your dataset as follows:
     *  * For 'csv' files, the header names are required, and we will extract the
     *    corresponding feature values when the header names align with the
     *    feature names.
     *  * For 'jsonl' files, we will extract the corresponding feature values if
     *    the key names match the feature names.
     *    Note: Nested features are not supported, so please ensure your features
     *    are flattened. Ensure the feature values are scalar or an array of
     *    scalars.
     *  * For 'bigquery' dataset, we will extract the corresponding feature values
     *    if the column names match the feature names.
     *    Note: The column type can be a scalar or an array of scalars. STRUCT or
     *    JSON types are not supported. You may use SQL queries to select or
     *    aggregate the relevant features from your original table. However,
     *    ensure that the 'schema' of the query results meets our requirements.
     *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
     *    Batch Prediction Job results. If the
     *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
     *    is an array, ensure that the sequence in
     *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
     *    matches the order of features in the prediction instance. We will match
     *    the feature with the array in the order specified in [feature_fields].
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
     * </code>
     */
    public java.util.List<
            ? extends
                com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder>
        getFeatureFieldsOrBuilderList() {
      if (featureFieldsBuilder_ != null) {
        return featureFieldsBuilder_.getMessageOrBuilderList();
      } else {
        return java.util.Collections.unmodifiableList(featureFields_);
      }
    }
    /**
     *
     *
     * <pre>
     * Feature names of the model. Vertex AI will try to match the features from
     * your dataset as follows:
     *  * For 'csv' files, the header names are required, and we will extract the
     *    corresponding feature values when the header names align with the
     *    feature names.
     *  * For 'jsonl' files, we will extract the corresponding feature values if
     *    the key names match the feature names.
     *    Note: Nested features are not supported, so please ensure your features
     *    are flattened. Ensure the feature values are scalar or an array of
     *    scalars.
     *  * For 'bigquery' dataset, we will extract the corresponding feature values
     *    if the column names match the feature names.
     *    Note: The column type can be a scalar or an array of scalars. STRUCT or
     *    JSON types are not supported. You may use SQL queries to select or
     *    aggregate the relevant features from your original table. However,
     *    ensure that the 'schema' of the query results meets our requirements.
     *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
     *    Batch Prediction Job results. If the
     *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
     *    is an array, ensure that the sequence in
     *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
     *    matches the order of features in the prediction instance. We will match
     *    the feature with the array in the order specified in [feature_fields].
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
     * </code>
     */
    public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder
        addFeatureFieldsBuilder() {
      return getFeatureFieldsFieldBuilder()
          .addBuilder(
              com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema
                  .getDefaultInstance());
    }
    /**
     *
     *
     * <pre>
     * Feature names of the model. Vertex AI will try to match the features from
     * your dataset as follows:
     *  * For 'csv' files, the header names are required, and we will extract the
     *    corresponding feature values when the header names align with the
     *    feature names.
     *  * For 'jsonl' files, we will extract the corresponding feature values if
     *    the key names match the feature names.
     *    Note: Nested features are not supported, so please ensure your features
     *    are flattened. Ensure the feature values are scalar or an array of
     *    scalars.
     *  * For 'bigquery' dataset, we will extract the corresponding feature values
     *    if the column names match the feature names.
     *    Note: The column type can be a scalar or an array of scalars. STRUCT or
     *    JSON types are not supported. You may use SQL queries to select or
     *    aggregate the relevant features from your original table. However,
     *    ensure that the 'schema' of the query results meets our requirements.
     *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
     *    Batch Prediction Job results. If the
     *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
     *    is an array, ensure that the sequence in
     *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
     *    matches the order of features in the prediction instance. We will match
     *    the feature with the array in the order specified in [feature_fields].
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
     * </code>
     */
    public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder
        addFeatureFieldsBuilder(int index) {
      return getFeatureFieldsFieldBuilder()
          .addBuilder(
              index,
              com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema
                  .getDefaultInstance());
    }
    /**
     *
     *
     * <pre>
     * Feature names of the model. Vertex AI will try to match the features from
     * your dataset as follows:
     *  * For 'csv' files, the header names are required, and we will extract the
     *    corresponding feature values when the header names align with the
     *    feature names.
     *  * For 'jsonl' files, we will extract the corresponding feature values if
     *    the key names match the feature names.
     *    Note: Nested features are not supported, so please ensure your features
     *    are flattened. Ensure the feature values are scalar or an array of
     *    scalars.
     *  * For 'bigquery' dataset, we will extract the corresponding feature values
     *    if the column names match the feature names.
     *    Note: The column type can be a scalar or an array of scalars. STRUCT or
     *    JSON types are not supported. You may use SQL queries to select or
     *    aggregate the relevant features from your original table. However,
     *    ensure that the 'schema' of the query results meets our requirements.
     *  * For the Vertex AI Endpoint Request Response Logging table or Vertex AI
     *    Batch Prediction Job results. If the
     *    [instance_type][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.instance_type]
     *    is an array, ensure that the sequence in
     *    [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields]
     *    matches the order of features in the prediction instance. We will match
     *    the feature with the array in the order specified in [feature_fields].
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema feature_fields = 1;
     * </code>
     */
    public java.util.List<
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder>
        getFeatureFieldsBuilderList() {
      return getFeatureFieldsFieldBuilder().getBuilderList();
    }

    private com.google.protobuf.RepeatedFieldBuilderV3<
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema,
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder,
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder>
        getFeatureFieldsFieldBuilder() {
      if (featureFieldsBuilder_ == null) {
        featureFieldsBuilder_ =
            new com.google.protobuf.RepeatedFieldBuilderV3<
                com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema,
                com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder,
                com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder>(
                featureFields_,
                ((bitField0_ & 0x00000001) != 0),
                getParentForChildren(),
                isClean());
        featureFields_ = null;
      }
      return featureFieldsBuilder_;
    }

    private java.util.List<com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema>
        predictionFields_ = java.util.Collections.emptyList();

    private void ensurePredictionFieldsIsMutable() {
      if (!((bitField0_ & 0x00000002) != 0)) {
        predictionFields_ =
            new java.util.ArrayList<
                com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema>(
                predictionFields_);
        bitField0_ |= 0x00000002;
      }
    }

    private com.google.protobuf.RepeatedFieldBuilderV3<
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema,
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder,
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder>
        predictionFieldsBuilder_;

    /**
     *
     *
     * <pre>
     * Prediction output names of the model. The requirements are the same as the
     * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
     * For AutoML Tables, the prediction output name presented in schema will be:
     * `predicted_{target_column}`, the `target_column` is the one you specified
     * when you train the model.
     * For Prediction output drift analysis:
     *  * AutoML Classification, the distribution of the argmax label will be
     *    analyzed.
     *  * AutoML Regression, the distribution of the value will be analyzed.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
     * </code>
     */
    public java.util.List<com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema>
        getPredictionFieldsList() {
      if (predictionFieldsBuilder_ == null) {
        return java.util.Collections.unmodifiableList(predictionFields_);
      } else {
        return predictionFieldsBuilder_.getMessageList();
      }
    }
    /**
     *
     *
     * <pre>
     * Prediction output names of the model. The requirements are the same as the
     * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
     * For AutoML Tables, the prediction output name presented in schema will be:
     * `predicted_{target_column}`, the `target_column` is the one you specified
     * when you train the model.
     * For Prediction output drift analysis:
     *  * AutoML Classification, the distribution of the argmax label will be
     *    analyzed.
     *  * AutoML Regression, the distribution of the value will be analyzed.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
     * </code>
     */
    public int getPredictionFieldsCount() {
      if (predictionFieldsBuilder_ == null) {
        return predictionFields_.size();
      } else {
        return predictionFieldsBuilder_.getCount();
      }
    }
    /**
     *
     *
     * <pre>
     * Prediction output names of the model. The requirements are the same as the
     * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
     * For AutoML Tables, the prediction output name presented in schema will be:
     * `predicted_{target_column}`, the `target_column` is the one you specified
     * when you train the model.
     * For Prediction output drift analysis:
     *  * AutoML Classification, the distribution of the argmax label will be
     *    analyzed.
     *  * AutoML Regression, the distribution of the value will be analyzed.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
     * </code>
     */
    public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema
        getPredictionFields(int index) {
      if (predictionFieldsBuilder_ == null) {
        return predictionFields_.get(index);
      } else {
        return predictionFieldsBuilder_.getMessage(index);
      }
    }
    /**
     *
     *
     * <pre>
     * Prediction output names of the model. The requirements are the same as the
     * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
     * For AutoML Tables, the prediction output name presented in schema will be:
     * `predicted_{target_column}`, the `target_column` is the one you specified
     * when you train the model.
     * For Prediction output drift analysis:
     *  * AutoML Classification, the distribution of the argmax label will be
     *    analyzed.
     *  * AutoML Regression, the distribution of the value will be analyzed.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
     * </code>
     */
    public Builder setPredictionFields(
        int index, com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema value) {
      if (predictionFieldsBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensurePredictionFieldsIsMutable();
        predictionFields_.set(index, value);
        onChanged();
      } else {
        predictionFieldsBuilder_.setMessage(index, value);
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Prediction output names of the model. The requirements are the same as the
     * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
     * For AutoML Tables, the prediction output name presented in schema will be:
     * `predicted_{target_column}`, the `target_column` is the one you specified
     * when you train the model.
     * For Prediction output drift analysis:
     *  * AutoML Classification, the distribution of the argmax label will be
     *    analyzed.
     *  * AutoML Regression, the distribution of the value will be analyzed.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
     * </code>
     */
    public Builder setPredictionFields(
        int index,
        com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder
            builderForValue) {
      if (predictionFieldsBuilder_ == null) {
        ensurePredictionFieldsIsMutable();
        predictionFields_.set(index, builderForValue.build());
        onChanged();
      } else {
        predictionFieldsBuilder_.setMessage(index, builderForValue.build());
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Prediction output names of the model. The requirements are the same as the
     * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
     * For AutoML Tables, the prediction output name presented in schema will be:
     * `predicted_{target_column}`, the `target_column` is the one you specified
     * when you train the model.
     * For Prediction output drift analysis:
     *  * AutoML Classification, the distribution of the argmax label will be
     *    analyzed.
     *  * AutoML Regression, the distribution of the value will be analyzed.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
     * </code>
     */
    public Builder addPredictionFields(
        com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema value) {
      if (predictionFieldsBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensurePredictionFieldsIsMutable();
        predictionFields_.add(value);
        onChanged();
      } else {
        predictionFieldsBuilder_.addMessage(value);
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Prediction output names of the model. The requirements are the same as the
     * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
     * For AutoML Tables, the prediction output name presented in schema will be:
     * `predicted_{target_column}`, the `target_column` is the one you specified
     * when you train the model.
     * For Prediction output drift analysis:
     *  * AutoML Classification, the distribution of the argmax label will be
     *    analyzed.
     *  * AutoML Regression, the distribution of the value will be analyzed.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
     * </code>
     */
    public Builder addPredictionFields(
        int index, com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema value) {
      if (predictionFieldsBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensurePredictionFieldsIsMutable();
        predictionFields_.add(index, value);
        onChanged();
      } else {
        predictionFieldsBuilder_.addMessage(index, value);
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Prediction output names of the model. The requirements are the same as the
     * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
     * For AutoML Tables, the prediction output name presented in schema will be:
     * `predicted_{target_column}`, the `target_column` is the one you specified
     * when you train the model.
     * For Prediction output drift analysis:
     *  * AutoML Classification, the distribution of the argmax label will be
     *    analyzed.
     *  * AutoML Regression, the distribution of the value will be analyzed.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
     * </code>
     */
    public Builder addPredictionFields(
        com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder
            builderForValue) {
      if (predictionFieldsBuilder_ == null) {
        ensurePredictionFieldsIsMutable();
        predictionFields_.add(builderForValue.build());
        onChanged();
      } else {
        predictionFieldsBuilder_.addMessage(builderForValue.build());
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Prediction output names of the model. The requirements are the same as the
     * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
     * For AutoML Tables, the prediction output name presented in schema will be:
     * `predicted_{target_column}`, the `target_column` is the one you specified
     * when you train the model.
     * For Prediction output drift analysis:
     *  * AutoML Classification, the distribution of the argmax label will be
     *    analyzed.
     *  * AutoML Regression, the distribution of the value will be analyzed.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
     * </code>
     */
    public Builder addPredictionFields(
        int index,
        com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder
            builderForValue) {
      if (predictionFieldsBuilder_ == null) {
        ensurePredictionFieldsIsMutable();
        predictionFields_.add(index, builderForValue.build());
        onChanged();
      } else {
        predictionFieldsBuilder_.addMessage(index, builderForValue.build());
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Prediction output names of the model. The requirements are the same as the
     * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
     * For AutoML Tables, the prediction output name presented in schema will be:
     * `predicted_{target_column}`, the `target_column` is the one you specified
     * when you train the model.
     * For Prediction output drift analysis:
     *  * AutoML Classification, the distribution of the argmax label will be
     *    analyzed.
     *  * AutoML Regression, the distribution of the value will be analyzed.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
     * </code>
     */
    public Builder addAllPredictionFields(
        java.lang.Iterable<
                ? extends com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema>
            values) {
      if (predictionFieldsBuilder_ == null) {
        ensurePredictionFieldsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(values, predictionFields_);
        onChanged();
      } else {
        predictionFieldsBuilder_.addAllMessages(values);
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Prediction output names of the model. The requirements are the same as the
     * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
     * For AutoML Tables, the prediction output name presented in schema will be:
     * `predicted_{target_column}`, the `target_column` is the one you specified
     * when you train the model.
     * For Prediction output drift analysis:
     *  * AutoML Classification, the distribution of the argmax label will be
     *    analyzed.
     *  * AutoML Regression, the distribution of the value will be analyzed.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
     * </code>
     */
    public Builder clearPredictionFields() {
      if (predictionFieldsBuilder_ == null) {
        predictionFields_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
      } else {
        predictionFieldsBuilder_.clear();
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Prediction output names of the model. The requirements are the same as the
     * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
     * For AutoML Tables, the prediction output name presented in schema will be:
     * `predicted_{target_column}`, the `target_column` is the one you specified
     * when you train the model.
     * For Prediction output drift analysis:
     *  * AutoML Classification, the distribution of the argmax label will be
     *    analyzed.
     *  * AutoML Regression, the distribution of the value will be analyzed.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
     * </code>
     */
    public Builder removePredictionFields(int index) {
      if (predictionFieldsBuilder_ == null) {
        ensurePredictionFieldsIsMutable();
        predictionFields_.remove(index);
        onChanged();
      } else {
        predictionFieldsBuilder_.remove(index);
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Prediction output names of the model. The requirements are the same as the
     * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
     * For AutoML Tables, the prediction output name presented in schema will be:
     * `predicted_{target_column}`, the `target_column` is the one you specified
     * when you train the model.
     * For Prediction output drift analysis:
     *  * AutoML Classification, the distribution of the argmax label will be
     *    analyzed.
     *  * AutoML Regression, the distribution of the value will be analyzed.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
     * </code>
     */
    public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder
        getPredictionFieldsBuilder(int index) {
      return getPredictionFieldsFieldBuilder().getBuilder(index);
    }
    /**
     *
     *
     * <pre>
     * Prediction output names of the model. The requirements are the same as the
     * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
     * For AutoML Tables, the prediction output name presented in schema will be:
     * `predicted_{target_column}`, the `target_column` is the one you specified
     * when you train the model.
     * For Prediction output drift analysis:
     *  * AutoML Classification, the distribution of the argmax label will be
     *    analyzed.
     *  * AutoML Regression, the distribution of the value will be analyzed.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
     * </code>
     */
    public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder
        getPredictionFieldsOrBuilder(int index) {
      if (predictionFieldsBuilder_ == null) {
        return predictionFields_.get(index);
      } else {
        return predictionFieldsBuilder_.getMessageOrBuilder(index);
      }
    }
    /**
     *
     *
     * <pre>
     * Prediction output names of the model. The requirements are the same as the
     * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
     * For AutoML Tables, the prediction output name presented in schema will be:
     * `predicted_{target_column}`, the `target_column` is the one you specified
     * when you train the model.
     * For Prediction output drift analysis:
     *  * AutoML Classification, the distribution of the argmax label will be
     *    analyzed.
     *  * AutoML Regression, the distribution of the value will be analyzed.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
     * </code>
     */
    public java.util.List<
            ? extends
                com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder>
        getPredictionFieldsOrBuilderList() {
      if (predictionFieldsBuilder_ != null) {
        return predictionFieldsBuilder_.getMessageOrBuilderList();
      } else {
        return java.util.Collections.unmodifiableList(predictionFields_);
      }
    }
    /**
     *
     *
     * <pre>
     * Prediction output names of the model. The requirements are the same as the
     * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
     * For AutoML Tables, the prediction output name presented in schema will be:
     * `predicted_{target_column}`, the `target_column` is the one you specified
     * when you train the model.
     * For Prediction output drift analysis:
     *  * AutoML Classification, the distribution of the argmax label will be
     *    analyzed.
     *  * AutoML Regression, the distribution of the value will be analyzed.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
     * </code>
     */
    public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder
        addPredictionFieldsBuilder() {
      return getPredictionFieldsFieldBuilder()
          .addBuilder(
              com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema
                  .getDefaultInstance());
    }
    /**
     *
     *
     * <pre>
     * Prediction output names of the model. The requirements are the same as the
     * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
     * For AutoML Tables, the prediction output name presented in schema will be:
     * `predicted_{target_column}`, the `target_column` is the one you specified
     * when you train the model.
     * For Prediction output drift analysis:
     *  * AutoML Classification, the distribution of the argmax label will be
     *    analyzed.
     *  * AutoML Regression, the distribution of the value will be analyzed.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
     * </code>
     */
    public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder
        addPredictionFieldsBuilder(int index) {
      return getPredictionFieldsFieldBuilder()
          .addBuilder(
              index,
              com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema
                  .getDefaultInstance());
    }
    /**
     *
     *
     * <pre>
     * Prediction output names of the model. The requirements are the same as the
     * [feature_fields][google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.feature_fields].
     * For AutoML Tables, the prediction output name presented in schema will be:
     * `predicted_{target_column}`, the `target_column` is the one you specified
     * when you train the model.
     * For Prediction output drift analysis:
     *  * AutoML Classification, the distribution of the argmax label will be
     *    analyzed.
     *  * AutoML Regression, the distribution of the value will be analyzed.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema prediction_fields = 2;
     * </code>
     */
    public java.util.List<
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder>
        getPredictionFieldsBuilderList() {
      return getPredictionFieldsFieldBuilder().getBuilderList();
    }

    private com.google.protobuf.RepeatedFieldBuilderV3<
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema,
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder,
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder>
        getPredictionFieldsFieldBuilder() {
      if (predictionFieldsBuilder_ == null) {
        predictionFieldsBuilder_ =
            new com.google.protobuf.RepeatedFieldBuilderV3<
                com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema,
                com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder,
                com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder>(
                predictionFields_,
                ((bitField0_ & 0x00000002) != 0),
                getParentForChildren(),
                isClean());
        predictionFields_ = null;
      }
      return predictionFieldsBuilder_;
    }

    private java.util.List<com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema>
        groundTruthFields_ = java.util.Collections.emptyList();

    private void ensureGroundTruthFieldsIsMutable() {
      if (!((bitField0_ & 0x00000004) != 0)) {
        groundTruthFields_ =
            new java.util.ArrayList<
                com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema>(
                groundTruthFields_);
        bitField0_ |= 0x00000004;
      }
    }

    private com.google.protobuf.RepeatedFieldBuilderV3<
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema,
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder,
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder>
        groundTruthFieldsBuilder_;

    /**
     *
     *
     * <pre>
     * Target /ground truth names of the model.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
     * </code>
     */
    public java.util.List<com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema>
        getGroundTruthFieldsList() {
      if (groundTruthFieldsBuilder_ == null) {
        return java.util.Collections.unmodifiableList(groundTruthFields_);
      } else {
        return groundTruthFieldsBuilder_.getMessageList();
      }
    }
    /**
     *
     *
     * <pre>
     * Target /ground truth names of the model.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
     * </code>
     */
    public int getGroundTruthFieldsCount() {
      if (groundTruthFieldsBuilder_ == null) {
        return groundTruthFields_.size();
      } else {
        return groundTruthFieldsBuilder_.getCount();
      }
    }
    /**
     *
     *
     * <pre>
     * Target /ground truth names of the model.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
     * </code>
     */
    public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema
        getGroundTruthFields(int index) {
      if (groundTruthFieldsBuilder_ == null) {
        return groundTruthFields_.get(index);
      } else {
        return groundTruthFieldsBuilder_.getMessage(index);
      }
    }
    /**
     *
     *
     * <pre>
     * Target /ground truth names of the model.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
     * </code>
     */
    public Builder setGroundTruthFields(
        int index, com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema value) {
      if (groundTruthFieldsBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureGroundTruthFieldsIsMutable();
        groundTruthFields_.set(index, value);
        onChanged();
      } else {
        groundTruthFieldsBuilder_.setMessage(index, value);
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Target /ground truth names of the model.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
     * </code>
     */
    public Builder setGroundTruthFields(
        int index,
        com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder
            builderForValue) {
      if (groundTruthFieldsBuilder_ == null) {
        ensureGroundTruthFieldsIsMutable();
        groundTruthFields_.set(index, builderForValue.build());
        onChanged();
      } else {
        groundTruthFieldsBuilder_.setMessage(index, builderForValue.build());
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Target /ground truth names of the model.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
     * </code>
     */
    public Builder addGroundTruthFields(
        com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema value) {
      if (groundTruthFieldsBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureGroundTruthFieldsIsMutable();
        groundTruthFields_.add(value);
        onChanged();
      } else {
        groundTruthFieldsBuilder_.addMessage(value);
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Target /ground truth names of the model.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
     * </code>
     */
    public Builder addGroundTruthFields(
        int index, com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema value) {
      if (groundTruthFieldsBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureGroundTruthFieldsIsMutable();
        groundTruthFields_.add(index, value);
        onChanged();
      } else {
        groundTruthFieldsBuilder_.addMessage(index, value);
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Target /ground truth names of the model.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
     * </code>
     */
    public Builder addGroundTruthFields(
        com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder
            builderForValue) {
      if (groundTruthFieldsBuilder_ == null) {
        ensureGroundTruthFieldsIsMutable();
        groundTruthFields_.add(builderForValue.build());
        onChanged();
      } else {
        groundTruthFieldsBuilder_.addMessage(builderForValue.build());
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Target /ground truth names of the model.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
     * </code>
     */
    public Builder addGroundTruthFields(
        int index,
        com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder
            builderForValue) {
      if (groundTruthFieldsBuilder_ == null) {
        ensureGroundTruthFieldsIsMutable();
        groundTruthFields_.add(index, builderForValue.build());
        onChanged();
      } else {
        groundTruthFieldsBuilder_.addMessage(index, builderForValue.build());
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Target /ground truth names of the model.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
     * </code>
     */
    public Builder addAllGroundTruthFields(
        java.lang.Iterable<
                ? extends com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema>
            values) {
      if (groundTruthFieldsBuilder_ == null) {
        ensureGroundTruthFieldsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(values, groundTruthFields_);
        onChanged();
      } else {
        groundTruthFieldsBuilder_.addAllMessages(values);
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Target /ground truth names of the model.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
     * </code>
     */
    public Builder clearGroundTruthFields() {
      if (groundTruthFieldsBuilder_ == null) {
        groundTruthFields_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000004);
        onChanged();
      } else {
        groundTruthFieldsBuilder_.clear();
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Target /ground truth names of the model.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
     * </code>
     */
    public Builder removeGroundTruthFields(int index) {
      if (groundTruthFieldsBuilder_ == null) {
        ensureGroundTruthFieldsIsMutable();
        groundTruthFields_.remove(index);
        onChanged();
      } else {
        groundTruthFieldsBuilder_.remove(index);
      }
      return this;
    }
    /**
     *
     *
     * <pre>
     * Target /ground truth names of the model.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
     * </code>
     */
    public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder
        getGroundTruthFieldsBuilder(int index) {
      return getGroundTruthFieldsFieldBuilder().getBuilder(index);
    }
    /**
     *
     *
     * <pre>
     * Target /ground truth names of the model.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
     * </code>
     */
    public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder
        getGroundTruthFieldsOrBuilder(int index) {
      if (groundTruthFieldsBuilder_ == null) {
        return groundTruthFields_.get(index);
      } else {
        return groundTruthFieldsBuilder_.getMessageOrBuilder(index);
      }
    }
    /**
     *
     *
     * <pre>
     * Target /ground truth names of the model.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
     * </code>
     */
    public java.util.List<
            ? extends
                com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder>
        getGroundTruthFieldsOrBuilderList() {
      if (groundTruthFieldsBuilder_ != null) {
        return groundTruthFieldsBuilder_.getMessageOrBuilderList();
      } else {
        return java.util.Collections.unmodifiableList(groundTruthFields_);
      }
    }
    /**
     *
     *
     * <pre>
     * Target /ground truth names of the model.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
     * </code>
     */
    public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder
        addGroundTruthFieldsBuilder() {
      return getGroundTruthFieldsFieldBuilder()
          .addBuilder(
              com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema
                  .getDefaultInstance());
    }
    /**
     *
     *
     * <pre>
     * Target /ground truth names of the model.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
     * </code>
     */
    public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder
        addGroundTruthFieldsBuilder(int index) {
      return getGroundTruthFieldsFieldBuilder()
          .addBuilder(
              index,
              com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema
                  .getDefaultInstance());
    }
    /**
     *
     *
     * <pre>
     * Target /ground truth names of the model.
     * </pre>
     *
     * <code>
     * repeated .google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema ground_truth_fields = 3;
     * </code>
     */
    public java.util.List<
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder>
        getGroundTruthFieldsBuilderList() {
      return getGroundTruthFieldsFieldBuilder().getBuilderList();
    }

    private com.google.protobuf.RepeatedFieldBuilderV3<
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema,
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder,
            com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder>
        getGroundTruthFieldsFieldBuilder() {
      if (groundTruthFieldsBuilder_ == null) {
        groundTruthFieldsBuilder_ =
            new com.google.protobuf.RepeatedFieldBuilderV3<
                com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema,
                com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchema.Builder,
                com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema.FieldSchemaOrBuilder>(
                groundTruthFields_,
                ((bitField0_ & 0x00000004) != 0),
                getParentForChildren(),
                isClean());
        groundTruthFields_ = null;
      }
      return groundTruthFieldsBuilder_;
    }

    @java.lang.Override
    public final Builder setUnknownFields(final com.google.protobuf.UnknownFieldSet unknownFields) {
      return super.setUnknownFields(unknownFields);
    }

    @java.lang.Override
    public final Builder mergeUnknownFields(
        final com.google.protobuf.UnknownFieldSet unknownFields) {
      return super.mergeUnknownFields(unknownFields);
    }

    // @@protoc_insertion_point(builder_scope:google.cloud.aiplatform.v1beta1.ModelMonitoringSchema)
  }

  // @@protoc_insertion_point(class_scope:google.cloud.aiplatform.v1beta1.ModelMonitoringSchema)
  private static final com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema DEFAULT_INSTANCE;

  static {
    DEFAULT_INSTANCE = new com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema();
  }

  public static com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema getDefaultInstance() {
    return DEFAULT_INSTANCE;
  }

  private static final com.google.protobuf.Parser<ModelMonitoringSchema> PARSER =
      new com.google.protobuf.AbstractParser<ModelMonitoringSchema>() {
        @java.lang.Override
        public ModelMonitoringSchema parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          Builder builder = newBuilder();
          try {
            builder.mergeFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            throw e.setUnfinishedMessage(builder.buildPartial());
          } catch (com.google.protobuf.UninitializedMessageException e) {
            throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
          } catch (java.io.IOException e) {
            throw new com.google.protobuf.InvalidProtocolBufferException(e)
                .setUnfinishedMessage(builder.buildPartial());
          }
          return builder.buildPartial();
        }
      };

  public static com.google.protobuf.Parser<ModelMonitoringSchema> parser() {
    return PARSER;
  }

  @java.lang.Override
  public com.google.protobuf.Parser<ModelMonitoringSchema> getParserForType() {
    return PARSER;
  }

  @java.lang.Override
  public com.google.cloud.aiplatform.v1beta1.ModelMonitoringSchema getDefaultInstanceForType() {
    return DEFAULT_INSTANCE;
  }
}
