/*
 * Copyright 2020 Google LLC
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     https://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: google/cloud/asset/v1/asset_service.proto

package com.google.cloud.asset.v1;

/**
 *
 *
 * <pre>
 * A BigQuery destination for exporting assets to.
 * </pre>
 *
 * Protobuf type {@code google.cloud.asset.v1.BigQueryDestination}
 */
public final class BigQueryDestination extends com.google.protobuf.GeneratedMessageV3
    implements
    // @@protoc_insertion_point(message_implements:google.cloud.asset.v1.BigQueryDestination)
    BigQueryDestinationOrBuilder {
  private static final long serialVersionUID = 0L;
  // Use BigQueryDestination.newBuilder() to construct.
  private BigQueryDestination(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
    super(builder);
  }

  private BigQueryDestination() {
    dataset_ = "";
    table_ = "";
  }

  @java.lang.Override
  @SuppressWarnings({"unused"})
  protected java.lang.Object newInstance(UnusedPrivateParameter unused) {
    return new BigQueryDestination();
  }

  @java.lang.Override
  public final com.google.protobuf.UnknownFieldSet getUnknownFields() {
    return this.unknownFields;
  }

  private BigQueryDestination(
      com.google.protobuf.CodedInputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    this();
    if (extensionRegistry == null) {
      throw new java.lang.NullPointerException();
    }
    com.google.protobuf.UnknownFieldSet.Builder unknownFields =
        com.google.protobuf.UnknownFieldSet.newBuilder();
    try {
      boolean done = false;
      while (!done) {
        int tag = input.readTag();
        switch (tag) {
          case 0:
            done = true;
            break;
          case 10:
            {
              java.lang.String s = input.readStringRequireUtf8();

              dataset_ = s;
              break;
            }
          case 18:
            {
              java.lang.String s = input.readStringRequireUtf8();

              table_ = s;
              break;
            }
          case 24:
            {
              force_ = input.readBool();
              break;
            }
          case 34:
            {
              com.google.cloud.asset.v1.PartitionSpec.Builder subBuilder = null;
              if (partitionSpec_ != null) {
                subBuilder = partitionSpec_.toBuilder();
              }
              partitionSpec_ =
                  input.readMessage(
                      com.google.cloud.asset.v1.PartitionSpec.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(partitionSpec_);
                partitionSpec_ = subBuilder.buildPartial();
              }

              break;
            }
          case 40:
            {
              separateTablesPerAssetType_ = input.readBool();
              break;
            }
          default:
            {
              if (!parseUnknownField(input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
        }
      }
    } catch (com.google.protobuf.InvalidProtocolBufferException e) {
      throw e.setUnfinishedMessage(this);
    } catch (java.io.IOException e) {
      throw new com.google.protobuf.InvalidProtocolBufferException(e).setUnfinishedMessage(this);
    } finally {
      this.unknownFields = unknownFields.build();
      makeExtensionsImmutable();
    }
  }

  public static final com.google.protobuf.Descriptors.Descriptor getDescriptor() {
    return com.google.cloud.asset.v1.AssetServiceProto
        .internal_static_google_cloud_asset_v1_BigQueryDestination_descriptor;
  }

  @java.lang.Override
  protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internalGetFieldAccessorTable() {
    return com.google.cloud.asset.v1.AssetServiceProto
        .internal_static_google_cloud_asset_v1_BigQueryDestination_fieldAccessorTable
        .ensureFieldAccessorsInitialized(
            com.google.cloud.asset.v1.BigQueryDestination.class,
            com.google.cloud.asset.v1.BigQueryDestination.Builder.class);
  }

  public static final int DATASET_FIELD_NUMBER = 1;
  private volatile java.lang.Object dataset_;
  /**
   *
   *
   * <pre>
   * Required. The BigQuery dataset in format
   * "projects/projectId/datasets/datasetId", to which the snapshot result
   * should be exported. If this dataset does not exist, the export call returns
   * an INVALID_ARGUMENT error.
   * </pre>
   *
   * <code>string dataset = 1 [(.google.api.field_behavior) = REQUIRED];</code>
   *
   * @return The dataset.
   */
  @java.lang.Override
  public java.lang.String getDataset() {
    java.lang.Object ref = dataset_;
    if (ref instanceof java.lang.String) {
      return (java.lang.String) ref;
    } else {
      com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref;
      java.lang.String s = bs.toStringUtf8();
      dataset_ = s;
      return s;
    }
  }
  /**
   *
   *
   * <pre>
   * Required. The BigQuery dataset in format
   * "projects/projectId/datasets/datasetId", to which the snapshot result
   * should be exported. If this dataset does not exist, the export call returns
   * an INVALID_ARGUMENT error.
   * </pre>
   *
   * <code>string dataset = 1 [(.google.api.field_behavior) = REQUIRED];</code>
   *
   * @return The bytes for dataset.
   */
  @java.lang.Override
  public com.google.protobuf.ByteString getDatasetBytes() {
    java.lang.Object ref = dataset_;
    if (ref instanceof java.lang.String) {
      com.google.protobuf.ByteString b =
          com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);
      dataset_ = b;
      return b;
    } else {
      return (com.google.protobuf.ByteString) ref;
    }
  }

  public static final int TABLE_FIELD_NUMBER = 2;
  private volatile java.lang.Object table_;
  /**
   *
   *
   * <pre>
   * Required. The BigQuery table to which the snapshot result should be
   * written. If this table does not exist, a new table with the given name
   * will be created.
   * </pre>
   *
   * <code>string table = 2 [(.google.api.field_behavior) = REQUIRED];</code>
   *
   * @return The table.
   */
  @java.lang.Override
  public java.lang.String getTable() {
    java.lang.Object ref = table_;
    if (ref instanceof java.lang.String) {
      return (java.lang.String) ref;
    } else {
      com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref;
      java.lang.String s = bs.toStringUtf8();
      table_ = s;
      return s;
    }
  }
  /**
   *
   *
   * <pre>
   * Required. The BigQuery table to which the snapshot result should be
   * written. If this table does not exist, a new table with the given name
   * will be created.
   * </pre>
   *
   * <code>string table = 2 [(.google.api.field_behavior) = REQUIRED];</code>
   *
   * @return The bytes for table.
   */
  @java.lang.Override
  public com.google.protobuf.ByteString getTableBytes() {
    java.lang.Object ref = table_;
    if (ref instanceof java.lang.String) {
      com.google.protobuf.ByteString b =
          com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);
      table_ = b;
      return b;
    } else {
      return (com.google.protobuf.ByteString) ref;
    }
  }

  public static final int FORCE_FIELD_NUMBER = 3;
  private boolean force_;
  /**
   *
   *
   * <pre>
   * If the destination table already exists and this flag is `TRUE`, the
   * table will be overwritten by the contents of assets snapshot. If the flag
   * is `FALSE` or unset and the destination table already exists, the export
   * call returns an INVALID_ARGUMEMT error.
   * </pre>
   *
   * <code>bool force = 3;</code>
   *
   * @return The force.
   */
  @java.lang.Override
  public boolean getForce() {
    return force_;
  }

  public static final int PARTITION_SPEC_FIELD_NUMBER = 4;
  private com.google.cloud.asset.v1.PartitionSpec partitionSpec_;
  /**
   *
   *
   * <pre>
   * [partition_spec] determines whether to export to partitioned table(s) and
   * how to partition the data.
   * If [partition_spec] is unset or [partition_spec.partition_key] is unset or
   * `PARTITION_KEY_UNSPECIFIED`, the snapshot results will be exported to
   * non-partitioned table(s). [force] will decide whether to overwrite existing
   * table(s).
   * If [partition_spec] is specified. First, the snapshot results will be
   * written to partitioned table(s) with two additional timestamp columns,
   * readTime and requestTime, one of which will be the partition key. Secondly,
   * in the case when any destination table already exists, it will first try to
   * update existing table's schema as necessary by appending additional
   * columns. Then, if [force] is `TRUE`, the corresponding partition will be
   * overwritten by the snapshot results (data in different partitions will
   * remain intact); if [force] is unset or `FALSE`, it will append the data. An
   * error will be returned if the schema update or data appension fails.
   * </pre>
   *
   * <code>.google.cloud.asset.v1.PartitionSpec partition_spec = 4;</code>
   *
   * @return Whether the partitionSpec field is set.
   */
  @java.lang.Override
  public boolean hasPartitionSpec() {
    return partitionSpec_ != null;
  }
  /**
   *
   *
   * <pre>
   * [partition_spec] determines whether to export to partitioned table(s) and
   * how to partition the data.
   * If [partition_spec] is unset or [partition_spec.partition_key] is unset or
   * `PARTITION_KEY_UNSPECIFIED`, the snapshot results will be exported to
   * non-partitioned table(s). [force] will decide whether to overwrite existing
   * table(s).
   * If [partition_spec] is specified. First, the snapshot results will be
   * written to partitioned table(s) with two additional timestamp columns,
   * readTime and requestTime, one of which will be the partition key. Secondly,
   * in the case when any destination table already exists, it will first try to
   * update existing table's schema as necessary by appending additional
   * columns. Then, if [force] is `TRUE`, the corresponding partition will be
   * overwritten by the snapshot results (data in different partitions will
   * remain intact); if [force] is unset or `FALSE`, it will append the data. An
   * error will be returned if the schema update or data appension fails.
   * </pre>
   *
   * <code>.google.cloud.asset.v1.PartitionSpec partition_spec = 4;</code>
   *
   * @return The partitionSpec.
   */
  @java.lang.Override
  public com.google.cloud.asset.v1.PartitionSpec getPartitionSpec() {
    return partitionSpec_ == null
        ? com.google.cloud.asset.v1.PartitionSpec.getDefaultInstance()
        : partitionSpec_;
  }
  /**
   *
   *
   * <pre>
   * [partition_spec] determines whether to export to partitioned table(s) and
   * how to partition the data.
   * If [partition_spec] is unset or [partition_spec.partition_key] is unset or
   * `PARTITION_KEY_UNSPECIFIED`, the snapshot results will be exported to
   * non-partitioned table(s). [force] will decide whether to overwrite existing
   * table(s).
   * If [partition_spec] is specified. First, the snapshot results will be
   * written to partitioned table(s) with two additional timestamp columns,
   * readTime and requestTime, one of which will be the partition key. Secondly,
   * in the case when any destination table already exists, it will first try to
   * update existing table's schema as necessary by appending additional
   * columns. Then, if [force] is `TRUE`, the corresponding partition will be
   * overwritten by the snapshot results (data in different partitions will
   * remain intact); if [force] is unset or `FALSE`, it will append the data. An
   * error will be returned if the schema update or data appension fails.
   * </pre>
   *
   * <code>.google.cloud.asset.v1.PartitionSpec partition_spec = 4;</code>
   */
  @java.lang.Override
  public com.google.cloud.asset.v1.PartitionSpecOrBuilder getPartitionSpecOrBuilder() {
    return getPartitionSpec();
  }

  public static final int SEPARATE_TABLES_PER_ASSET_TYPE_FIELD_NUMBER = 5;
  private boolean separateTablesPerAssetType_;
  /**
   *
   *
   * <pre>
   * If this flag is `TRUE`, the snapshot results will be written to one or
   * multiple tables, each of which contains results of one asset type. The
   * [force] and [partition_spec] fields will apply to each of them.
   * Field [table] will be concatenated with "_" and the asset type names (see
   * https://cloud.google.com/asset-inventory/docs/supported-asset-types for
   * supported asset types) to construct per-asset-type table names, in which
   * all non-alphanumeric characters like "." and "/" will be substituted by
   * "_". Example: if field [table] is "mytable" and snapshot results
   * contain "storage.googleapis.com/Bucket" assets, the corresponding table
   * name will be "mytable_storage_googleapis_com_Bucket". If any of these
   * tables does not exist, a new table with the concatenated name will be
   * created.
   * When [content_type] in the ExportAssetsRequest is `RESOURCE`, the schema of
   * each table will include RECORD-type columns mapped to the nested fields in
   * the Asset.resource.data field of that asset type (up to the 15 nested level
   * BigQuery supports
   * (https://cloud.google.com/bigquery/docs/nested-repeated#limitations)). The
   * fields in &gt;15 nested levels will be stored in JSON format string as a child
   * column of its parent RECORD column.
   * If error occurs when exporting to any table, the whole export call will
   * return an error but the export results that already succeed will persist.
   * Example: if exporting to table_type_A succeeds when exporting to
   * table_type_B fails during one export call, the results in table_type_A will
   * persist and there will not be partial results persisting in a table.
   * </pre>
   *
   * <code>bool separate_tables_per_asset_type = 5;</code>
   *
   * @return The separateTablesPerAssetType.
   */
  @java.lang.Override
  public boolean getSeparateTablesPerAssetType() {
    return separateTablesPerAssetType_;
  }

  private byte memoizedIsInitialized = -1;

  @java.lang.Override
  public final boolean isInitialized() {
    byte isInitialized = memoizedIsInitialized;
    if (isInitialized == 1) return true;
    if (isInitialized == 0) return false;

    memoizedIsInitialized = 1;
    return true;
  }

  @java.lang.Override
  public void writeTo(com.google.protobuf.CodedOutputStream output) throws java.io.IOException {
    if (!getDatasetBytes().isEmpty()) {
      com.google.protobuf.GeneratedMessageV3.writeString(output, 1, dataset_);
    }
    if (!getTableBytes().isEmpty()) {
      com.google.protobuf.GeneratedMessageV3.writeString(output, 2, table_);
    }
    if (force_ != false) {
      output.writeBool(3, force_);
    }
    if (partitionSpec_ != null) {
      output.writeMessage(4, getPartitionSpec());
    }
    if (separateTablesPerAssetType_ != false) {
      output.writeBool(5, separateTablesPerAssetType_);
    }
    unknownFields.writeTo(output);
  }

  @java.lang.Override
  public int getSerializedSize() {
    int size = memoizedSize;
    if (size != -1) return size;

    size = 0;
    if (!getDatasetBytes().isEmpty()) {
      size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, dataset_);
    }
    if (!getTableBytes().isEmpty()) {
      size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, table_);
    }
    if (force_ != false) {
      size += com.google.protobuf.CodedOutputStream.computeBoolSize(3, force_);
    }
    if (partitionSpec_ != null) {
      size += com.google.protobuf.CodedOutputStream.computeMessageSize(4, getPartitionSpec());
    }
    if (separateTablesPerAssetType_ != false) {
      size += com.google.protobuf.CodedOutputStream.computeBoolSize(5, separateTablesPerAssetType_);
    }
    size += unknownFields.getSerializedSize();
    memoizedSize = size;
    return size;
  }

  @java.lang.Override
  public boolean equals(final java.lang.Object obj) {
    if (obj == this) {
      return true;
    }
    if (!(obj instanceof com.google.cloud.asset.v1.BigQueryDestination)) {
      return super.equals(obj);
    }
    com.google.cloud.asset.v1.BigQueryDestination other =
        (com.google.cloud.asset.v1.BigQueryDestination) obj;

    if (!getDataset().equals(other.getDataset())) return false;
    if (!getTable().equals(other.getTable())) return false;
    if (getForce() != other.getForce()) return false;
    if (hasPartitionSpec() != other.hasPartitionSpec()) return false;
    if (hasPartitionSpec()) {
      if (!getPartitionSpec().equals(other.getPartitionSpec())) return false;
    }
    if (getSeparateTablesPerAssetType() != other.getSeparateTablesPerAssetType()) return false;
    if (!unknownFields.equals(other.unknownFields)) return false;
    return true;
  }

  @java.lang.Override
  public int hashCode() {
    if (memoizedHashCode != 0) {
      return memoizedHashCode;
    }
    int hash = 41;
    hash = (19 * hash) + getDescriptor().hashCode();
    hash = (37 * hash) + DATASET_FIELD_NUMBER;
    hash = (53 * hash) + getDataset().hashCode();
    hash = (37 * hash) + TABLE_FIELD_NUMBER;
    hash = (53 * hash) + getTable().hashCode();
    hash = (37 * hash) + FORCE_FIELD_NUMBER;
    hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(getForce());
    if (hasPartitionSpec()) {
      hash = (37 * hash) + PARTITION_SPEC_FIELD_NUMBER;
      hash = (53 * hash) + getPartitionSpec().hashCode();
    }
    hash = (37 * hash) + SEPARATE_TABLES_PER_ASSET_TYPE_FIELD_NUMBER;
    hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(getSeparateTablesPerAssetType());
    hash = (29 * hash) + unknownFields.hashCode();
    memoizedHashCode = hash;
    return hash;
  }

  public static com.google.cloud.asset.v1.BigQueryDestination parseFrom(java.nio.ByteBuffer data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }

  public static com.google.cloud.asset.v1.BigQueryDestination parseFrom(
      java.nio.ByteBuffer data, com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }

  public static com.google.cloud.asset.v1.BigQueryDestination parseFrom(
      com.google.protobuf.ByteString data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }

  public static com.google.cloud.asset.v1.BigQueryDestination parseFrom(
      com.google.protobuf.ByteString data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }

  public static com.google.cloud.asset.v1.BigQueryDestination parseFrom(byte[] data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }

  public static com.google.cloud.asset.v1.BigQueryDestination parseFrom(
      byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }

  public static com.google.cloud.asset.v1.BigQueryDestination parseFrom(java.io.InputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3.parseWithIOException(PARSER, input);
  }

  public static com.google.cloud.asset.v1.BigQueryDestination parseFrom(
      java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3.parseWithIOException(
        PARSER, input, extensionRegistry);
  }

  public static com.google.cloud.asset.v1.BigQueryDestination parseDelimitedFrom(
      java.io.InputStream input) throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3.parseDelimitedWithIOException(PARSER, input);
  }

  public static com.google.cloud.asset.v1.BigQueryDestination parseDelimitedFrom(
      java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3.parseDelimitedWithIOException(
        PARSER, input, extensionRegistry);
  }

  public static com.google.cloud.asset.v1.BigQueryDestination parseFrom(
      com.google.protobuf.CodedInputStream input) throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3.parseWithIOException(PARSER, input);
  }

  public static com.google.cloud.asset.v1.BigQueryDestination parseFrom(
      com.google.protobuf.CodedInputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3.parseWithIOException(
        PARSER, input, extensionRegistry);
  }

  @java.lang.Override
  public Builder newBuilderForType() {
    return newBuilder();
  }

  public static Builder newBuilder() {
    return DEFAULT_INSTANCE.toBuilder();
  }

  public static Builder newBuilder(com.google.cloud.asset.v1.BigQueryDestination prototype) {
    return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
  }

  @java.lang.Override
  public Builder toBuilder() {
    return this == DEFAULT_INSTANCE ? new Builder() : new Builder().mergeFrom(this);
  }

  @java.lang.Override
  protected Builder newBuilderForType(com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
    Builder builder = new Builder(parent);
    return builder;
  }
  /**
   *
   *
   * <pre>
   * A BigQuery destination for exporting assets to.
   * </pre>
   *
   * Protobuf type {@code google.cloud.asset.v1.BigQueryDestination}
   */
  public static final class Builder extends com.google.protobuf.GeneratedMessageV3.Builder<Builder>
      implements
      // @@protoc_insertion_point(builder_implements:google.cloud.asset.v1.BigQueryDestination)
      com.google.cloud.asset.v1.BigQueryDestinationOrBuilder {
    public static final com.google.protobuf.Descriptors.Descriptor getDescriptor() {
      return com.google.cloud.asset.v1.AssetServiceProto
          .internal_static_google_cloud_asset_v1_BigQueryDestination_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return com.google.cloud.asset.v1.AssetServiceProto
          .internal_static_google_cloud_asset_v1_BigQueryDestination_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              com.google.cloud.asset.v1.BigQueryDestination.class,
              com.google.cloud.asset.v1.BigQueryDestination.Builder.class);
    }

    // Construct using com.google.cloud.asset.v1.BigQueryDestination.newBuilder()
    private Builder() {
      maybeForceBuilderInitialization();
    }

    private Builder(com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      super(parent);
      maybeForceBuilderInitialization();
    }

    private void maybeForceBuilderInitialization() {
      if (com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders) {}
    }

    @java.lang.Override
    public Builder clear() {
      super.clear();
      dataset_ = "";

      table_ = "";

      force_ = false;

      if (partitionSpecBuilder_ == null) {
        partitionSpec_ = null;
      } else {
        partitionSpec_ = null;
        partitionSpecBuilder_ = null;
      }
      separateTablesPerAssetType_ = false;

      return this;
    }

    @java.lang.Override
    public com.google.protobuf.Descriptors.Descriptor getDescriptorForType() {
      return com.google.cloud.asset.v1.AssetServiceProto
          .internal_static_google_cloud_asset_v1_BigQueryDestination_descriptor;
    }

    @java.lang.Override
    public com.google.cloud.asset.v1.BigQueryDestination getDefaultInstanceForType() {
      return com.google.cloud.asset.v1.BigQueryDestination.getDefaultInstance();
    }

    @java.lang.Override
    public com.google.cloud.asset.v1.BigQueryDestination build() {
      com.google.cloud.asset.v1.BigQueryDestination result = buildPartial();
      if (!result.isInitialized()) {
        throw newUninitializedMessageException(result);
      }
      return result;
    }

    @java.lang.Override
    public com.google.cloud.asset.v1.BigQueryDestination buildPartial() {
      com.google.cloud.asset.v1.BigQueryDestination result =
          new com.google.cloud.asset.v1.BigQueryDestination(this);
      result.dataset_ = dataset_;
      result.table_ = table_;
      result.force_ = force_;
      if (partitionSpecBuilder_ == null) {
        result.partitionSpec_ = partitionSpec_;
      } else {
        result.partitionSpec_ = partitionSpecBuilder_.build();
      }
      result.separateTablesPerAssetType_ = separateTablesPerAssetType_;
      onBuilt();
      return result;
    }

    @java.lang.Override
    public Builder clone() {
      return super.clone();
    }

    @java.lang.Override
    public Builder setField(
        com.google.protobuf.Descriptors.FieldDescriptor field, java.lang.Object value) {
      return super.setField(field, value);
    }

    @java.lang.Override
    public Builder clearField(com.google.protobuf.Descriptors.FieldDescriptor field) {
      return super.clearField(field);
    }

    @java.lang.Override
    public Builder clearOneof(com.google.protobuf.Descriptors.OneofDescriptor oneof) {
      return super.clearOneof(oneof);
    }

    @java.lang.Override
    public Builder setRepeatedField(
        com.google.protobuf.Descriptors.FieldDescriptor field, int index, java.lang.Object value) {
      return super.setRepeatedField(field, index, value);
    }

    @java.lang.Override
    public Builder addRepeatedField(
        com.google.protobuf.Descriptors.FieldDescriptor field, java.lang.Object value) {
      return super.addRepeatedField(field, value);
    }

    @java.lang.Override
    public Builder mergeFrom(com.google.protobuf.Message other) {
      if (other instanceof com.google.cloud.asset.v1.BigQueryDestination) {
        return mergeFrom((com.google.cloud.asset.v1.BigQueryDestination) other);
      } else {
        super.mergeFrom(other);
        return this;
      }
    }

    public Builder mergeFrom(com.google.cloud.asset.v1.BigQueryDestination other) {
      if (other == com.google.cloud.asset.v1.BigQueryDestination.getDefaultInstance()) return this;
      if (!other.getDataset().isEmpty()) {
        dataset_ = other.dataset_;
        onChanged();
      }
      if (!other.getTable().isEmpty()) {
        table_ = other.table_;
        onChanged();
      }
      if (other.getForce() != false) {
        setForce(other.getForce());
      }
      if (other.hasPartitionSpec()) {
        mergePartitionSpec(other.getPartitionSpec());
      }
      if (other.getSeparateTablesPerAssetType() != false) {
        setSeparateTablesPerAssetType(other.getSeparateTablesPerAssetType());
      }
      this.mergeUnknownFields(other.unknownFields);
      onChanged();
      return this;
    }

    @java.lang.Override
    public final boolean isInitialized() {
      return true;
    }

    @java.lang.Override
    public Builder mergeFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      com.google.cloud.asset.v1.BigQueryDestination parsedMessage = null;
      try {
        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        parsedMessage = (com.google.cloud.asset.v1.BigQueryDestination) e.getUnfinishedMessage();
        throw e.unwrapIOException();
      } finally {
        if (parsedMessage != null) {
          mergeFrom(parsedMessage);
        }
      }
      return this;
    }

    private java.lang.Object dataset_ = "";
    /**
     *
     *
     * <pre>
     * Required. The BigQuery dataset in format
     * "projects/projectId/datasets/datasetId", to which the snapshot result
     * should be exported. If this dataset does not exist, the export call returns
     * an INVALID_ARGUMENT error.
     * </pre>
     *
     * <code>string dataset = 1 [(.google.api.field_behavior) = REQUIRED];</code>
     *
     * @return The dataset.
     */
    public java.lang.String getDataset() {
      java.lang.Object ref = dataset_;
      if (!(ref instanceof java.lang.String)) {
        com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        dataset_ = s;
        return s;
      } else {
        return (java.lang.String) ref;
      }
    }
    /**
     *
     *
     * <pre>
     * Required. The BigQuery dataset in format
     * "projects/projectId/datasets/datasetId", to which the snapshot result
     * should be exported. If this dataset does not exist, the export call returns
     * an INVALID_ARGUMENT error.
     * </pre>
     *
     * <code>string dataset = 1 [(.google.api.field_behavior) = REQUIRED];</code>
     *
     * @return The bytes for dataset.
     */
    public com.google.protobuf.ByteString getDatasetBytes() {
      java.lang.Object ref = dataset_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b =
            com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);
        dataset_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    /**
     *
     *
     * <pre>
     * Required. The BigQuery dataset in format
     * "projects/projectId/datasets/datasetId", to which the snapshot result
     * should be exported. If this dataset does not exist, the export call returns
     * an INVALID_ARGUMENT error.
     * </pre>
     *
     * <code>string dataset = 1 [(.google.api.field_behavior) = REQUIRED];</code>
     *
     * @param value The dataset to set.
     * @return This builder for chaining.
     */
    public Builder setDataset(java.lang.String value) {
      if (value == null) {
        throw new NullPointerException();
      }

      dataset_ = value;
      onChanged();
      return this;
    }
    /**
     *
     *
     * <pre>
     * Required. The BigQuery dataset in format
     * "projects/projectId/datasets/datasetId", to which the snapshot result
     * should be exported. If this dataset does not exist, the export call returns
     * an INVALID_ARGUMENT error.
     * </pre>
     *
     * <code>string dataset = 1 [(.google.api.field_behavior) = REQUIRED];</code>
     *
     * @return This builder for chaining.
     */
    public Builder clearDataset() {

      dataset_ = getDefaultInstance().getDataset();
      onChanged();
      return this;
    }
    /**
     *
     *
     * <pre>
     * Required. The BigQuery dataset in format
     * "projects/projectId/datasets/datasetId", to which the snapshot result
     * should be exported. If this dataset does not exist, the export call returns
     * an INVALID_ARGUMENT error.
     * </pre>
     *
     * <code>string dataset = 1 [(.google.api.field_behavior) = REQUIRED];</code>
     *
     * @param value The bytes for dataset to set.
     * @return This builder for chaining.
     */
    public Builder setDatasetBytes(com.google.protobuf.ByteString value) {
      if (value == null) {
        throw new NullPointerException();
      }
      checkByteStringIsUtf8(value);

      dataset_ = value;
      onChanged();
      return this;
    }

    private java.lang.Object table_ = "";
    /**
     *
     *
     * <pre>
     * Required. The BigQuery table to which the snapshot result should be
     * written. If this table does not exist, a new table with the given name
     * will be created.
     * </pre>
     *
     * <code>string table = 2 [(.google.api.field_behavior) = REQUIRED];</code>
     *
     * @return The table.
     */
    public java.lang.String getTable() {
      java.lang.Object ref = table_;
      if (!(ref instanceof java.lang.String)) {
        com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        table_ = s;
        return s;
      } else {
        return (java.lang.String) ref;
      }
    }
    /**
     *
     *
     * <pre>
     * Required. The BigQuery table to which the snapshot result should be
     * written. If this table does not exist, a new table with the given name
     * will be created.
     * </pre>
     *
     * <code>string table = 2 [(.google.api.field_behavior) = REQUIRED];</code>
     *
     * @return The bytes for table.
     */
    public com.google.protobuf.ByteString getTableBytes() {
      java.lang.Object ref = table_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b =
            com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);
        table_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    /**
     *
     *
     * <pre>
     * Required. The BigQuery table to which the snapshot result should be
     * written. If this table does not exist, a new table with the given name
     * will be created.
     * </pre>
     *
     * <code>string table = 2 [(.google.api.field_behavior) = REQUIRED];</code>
     *
     * @param value The table to set.
     * @return This builder for chaining.
     */
    public Builder setTable(java.lang.String value) {
      if (value == null) {
        throw new NullPointerException();
      }

      table_ = value;
      onChanged();
      return this;
    }
    /**
     *
     *
     * <pre>
     * Required. The BigQuery table to which the snapshot result should be
     * written. If this table does not exist, a new table with the given name
     * will be created.
     * </pre>
     *
     * <code>string table = 2 [(.google.api.field_behavior) = REQUIRED];</code>
     *
     * @return This builder for chaining.
     */
    public Builder clearTable() {

      table_ = getDefaultInstance().getTable();
      onChanged();
      return this;
    }
    /**
     *
     *
     * <pre>
     * Required. The BigQuery table to which the snapshot result should be
     * written. If this table does not exist, a new table with the given name
     * will be created.
     * </pre>
     *
     * <code>string table = 2 [(.google.api.field_behavior) = REQUIRED];</code>
     *
     * @param value The bytes for table to set.
     * @return This builder for chaining.
     */
    public Builder setTableBytes(com.google.protobuf.ByteString value) {
      if (value == null) {
        throw new NullPointerException();
      }
      checkByteStringIsUtf8(value);

      table_ = value;
      onChanged();
      return this;
    }

    private boolean force_;
    /**
     *
     *
     * <pre>
     * If the destination table already exists and this flag is `TRUE`, the
     * table will be overwritten by the contents of assets snapshot. If the flag
     * is `FALSE` or unset and the destination table already exists, the export
     * call returns an INVALID_ARGUMEMT error.
     * </pre>
     *
     * <code>bool force = 3;</code>
     *
     * @return The force.
     */
    @java.lang.Override
    public boolean getForce() {
      return force_;
    }
    /**
     *
     *
     * <pre>
     * If the destination table already exists and this flag is `TRUE`, the
     * table will be overwritten by the contents of assets snapshot. If the flag
     * is `FALSE` or unset and the destination table already exists, the export
     * call returns an INVALID_ARGUMEMT error.
     * </pre>
     *
     * <code>bool force = 3;</code>
     *
     * @param value The force to set.
     * @return This builder for chaining.
     */
    public Builder setForce(boolean value) {

      force_ = value;
      onChanged();
      return this;
    }
    /**
     *
     *
     * <pre>
     * If the destination table already exists and this flag is `TRUE`, the
     * table will be overwritten by the contents of assets snapshot. If the flag
     * is `FALSE` or unset and the destination table already exists, the export
     * call returns an INVALID_ARGUMEMT error.
     * </pre>
     *
     * <code>bool force = 3;</code>
     *
     * @return This builder for chaining.
     */
    public Builder clearForce() {

      force_ = false;
      onChanged();
      return this;
    }

    private com.google.cloud.asset.v1.PartitionSpec partitionSpec_;
    private com.google.protobuf.SingleFieldBuilderV3<
            com.google.cloud.asset.v1.PartitionSpec,
            com.google.cloud.asset.v1.PartitionSpec.Builder,
            com.google.cloud.asset.v1.PartitionSpecOrBuilder>
        partitionSpecBuilder_;
    /**
     *
     *
     * <pre>
     * [partition_spec] determines whether to export to partitioned table(s) and
     * how to partition the data.
     * If [partition_spec] is unset or [partition_spec.partition_key] is unset or
     * `PARTITION_KEY_UNSPECIFIED`, the snapshot results will be exported to
     * non-partitioned table(s). [force] will decide whether to overwrite existing
     * table(s).
     * If [partition_spec] is specified. First, the snapshot results will be
     * written to partitioned table(s) with two additional timestamp columns,
     * readTime and requestTime, one of which will be the partition key. Secondly,
     * in the case when any destination table already exists, it will first try to
     * update existing table's schema as necessary by appending additional
     * columns. Then, if [force] is `TRUE`, the corresponding partition will be
     * overwritten by the snapshot results (data in different partitions will
     * remain intact); if [force] is unset or `FALSE`, it will append the data. An
     * error will be returned if the schema update or data appension fails.
     * </pre>
     *
     * <code>.google.cloud.asset.v1.PartitionSpec partition_spec = 4;</code>
     *
     * @return Whether the partitionSpec field is set.
     */
    public boolean hasPartitionSpec() {
      return partitionSpecBuilder_ != null || partitionSpec_ != null;
    }
    /**
     *
     *
     * <pre>
     * [partition_spec] determines whether to export to partitioned table(s) and
     * how to partition the data.
     * If [partition_spec] is unset or [partition_spec.partition_key] is unset or
     * `PARTITION_KEY_UNSPECIFIED`, the snapshot results will be exported to
     * non-partitioned table(s). [force] will decide whether to overwrite existing
     * table(s).
     * If [partition_spec] is specified. First, the snapshot results will be
     * written to partitioned table(s) with two additional timestamp columns,
     * readTime and requestTime, one of which will be the partition key. Secondly,
     * in the case when any destination table already exists, it will first try to
     * update existing table's schema as necessary by appending additional
     * columns. Then, if [force] is `TRUE`, the corresponding partition will be
     * overwritten by the snapshot results (data in different partitions will
     * remain intact); if [force] is unset or `FALSE`, it will append the data. An
     * error will be returned if the schema update or data appension fails.
     * </pre>
     *
     * <code>.google.cloud.asset.v1.PartitionSpec partition_spec = 4;</code>
     *
     * @return The partitionSpec.
     */
    public com.google.cloud.asset.v1.PartitionSpec getPartitionSpec() {
      if (partitionSpecBuilder_ == null) {
        return partitionSpec_ == null
            ? com.google.cloud.asset.v1.PartitionSpec.getDefaultInstance()
            : partitionSpec_;
      } else {
        return partitionSpecBuilder_.getMessage();
      }
    }
    /**
     *
     *
     * <pre>
     * [partition_spec] determines whether to export to partitioned table(s) and
     * how to partition the data.
     * If [partition_spec] is unset or [partition_spec.partition_key] is unset or
     * `PARTITION_KEY_UNSPECIFIED`, the snapshot results will be exported to
     * non-partitioned table(s). [force] will decide whether to overwrite existing
     * table(s).
     * If [partition_spec] is specified. First, the snapshot results will be
     * written to partitioned table(s) with two additional timestamp columns,
     * readTime and requestTime, one of which will be the partition key. Secondly,
     * in the case when any destination table already exists, it will first try to
     * update existing table's schema as necessary by appending additional
     * columns. Then, if [force] is `TRUE`, the corresponding partition will be
     * overwritten by the snapshot results (data in different partitions will
     * remain intact); if [force] is unset or `FALSE`, it will append the data. An
     * error will be returned if the schema update or data appension fails.
     * </pre>
     *
     * <code>.google.cloud.asset.v1.PartitionSpec partition_spec = 4;</code>
     */
    public Builder setPartitionSpec(com.google.cloud.asset.v1.PartitionSpec value) {
      if (partitionSpecBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        partitionSpec_ = value;
        onChanged();
      } else {
        partitionSpecBuilder_.setMessage(value);
      }

      return this;
    }
    /**
     *
     *
     * <pre>
     * [partition_spec] determines whether to export to partitioned table(s) and
     * how to partition the data.
     * If [partition_spec] is unset or [partition_spec.partition_key] is unset or
     * `PARTITION_KEY_UNSPECIFIED`, the snapshot results will be exported to
     * non-partitioned table(s). [force] will decide whether to overwrite existing
     * table(s).
     * If [partition_spec] is specified. First, the snapshot results will be
     * written to partitioned table(s) with two additional timestamp columns,
     * readTime and requestTime, one of which will be the partition key. Secondly,
     * in the case when any destination table already exists, it will first try to
     * update existing table's schema as necessary by appending additional
     * columns. Then, if [force] is `TRUE`, the corresponding partition will be
     * overwritten by the snapshot results (data in different partitions will
     * remain intact); if [force] is unset or `FALSE`, it will append the data. An
     * error will be returned if the schema update or data appension fails.
     * </pre>
     *
     * <code>.google.cloud.asset.v1.PartitionSpec partition_spec = 4;</code>
     */
    public Builder setPartitionSpec(
        com.google.cloud.asset.v1.PartitionSpec.Builder builderForValue) {
      if (partitionSpecBuilder_ == null) {
        partitionSpec_ = builderForValue.build();
        onChanged();
      } else {
        partitionSpecBuilder_.setMessage(builderForValue.build());
      }

      return this;
    }
    /**
     *
     *
     * <pre>
     * [partition_spec] determines whether to export to partitioned table(s) and
     * how to partition the data.
     * If [partition_spec] is unset or [partition_spec.partition_key] is unset or
     * `PARTITION_KEY_UNSPECIFIED`, the snapshot results will be exported to
     * non-partitioned table(s). [force] will decide whether to overwrite existing
     * table(s).
     * If [partition_spec] is specified. First, the snapshot results will be
     * written to partitioned table(s) with two additional timestamp columns,
     * readTime and requestTime, one of which will be the partition key. Secondly,
     * in the case when any destination table already exists, it will first try to
     * update existing table's schema as necessary by appending additional
     * columns. Then, if [force] is `TRUE`, the corresponding partition will be
     * overwritten by the snapshot results (data in different partitions will
     * remain intact); if [force] is unset or `FALSE`, it will append the data. An
     * error will be returned if the schema update or data appension fails.
     * </pre>
     *
     * <code>.google.cloud.asset.v1.PartitionSpec partition_spec = 4;</code>
     */
    public Builder mergePartitionSpec(com.google.cloud.asset.v1.PartitionSpec value) {
      if (partitionSpecBuilder_ == null) {
        if (partitionSpec_ != null) {
          partitionSpec_ =
              com.google.cloud.asset.v1.PartitionSpec.newBuilder(partitionSpec_)
                  .mergeFrom(value)
                  .buildPartial();
        } else {
          partitionSpec_ = value;
        }
        onChanged();
      } else {
        partitionSpecBuilder_.mergeFrom(value);
      }

      return this;
    }
    /**
     *
     *
     * <pre>
     * [partition_spec] determines whether to export to partitioned table(s) and
     * how to partition the data.
     * If [partition_spec] is unset or [partition_spec.partition_key] is unset or
     * `PARTITION_KEY_UNSPECIFIED`, the snapshot results will be exported to
     * non-partitioned table(s). [force] will decide whether to overwrite existing
     * table(s).
     * If [partition_spec] is specified. First, the snapshot results will be
     * written to partitioned table(s) with two additional timestamp columns,
     * readTime and requestTime, one of which will be the partition key. Secondly,
     * in the case when any destination table already exists, it will first try to
     * update existing table's schema as necessary by appending additional
     * columns. Then, if [force] is `TRUE`, the corresponding partition will be
     * overwritten by the snapshot results (data in different partitions will
     * remain intact); if [force] is unset or `FALSE`, it will append the data. An
     * error will be returned if the schema update or data appension fails.
     * </pre>
     *
     * <code>.google.cloud.asset.v1.PartitionSpec partition_spec = 4;</code>
     */
    public Builder clearPartitionSpec() {
      if (partitionSpecBuilder_ == null) {
        partitionSpec_ = null;
        onChanged();
      } else {
        partitionSpec_ = null;
        partitionSpecBuilder_ = null;
      }

      return this;
    }
    /**
     *
     *
     * <pre>
     * [partition_spec] determines whether to export to partitioned table(s) and
     * how to partition the data.
     * If [partition_spec] is unset or [partition_spec.partition_key] is unset or
     * `PARTITION_KEY_UNSPECIFIED`, the snapshot results will be exported to
     * non-partitioned table(s). [force] will decide whether to overwrite existing
     * table(s).
     * If [partition_spec] is specified. First, the snapshot results will be
     * written to partitioned table(s) with two additional timestamp columns,
     * readTime and requestTime, one of which will be the partition key. Secondly,
     * in the case when any destination table already exists, it will first try to
     * update existing table's schema as necessary by appending additional
     * columns. Then, if [force] is `TRUE`, the corresponding partition will be
     * overwritten by the snapshot results (data in different partitions will
     * remain intact); if [force] is unset or `FALSE`, it will append the data. An
     * error will be returned if the schema update or data appension fails.
     * </pre>
     *
     * <code>.google.cloud.asset.v1.PartitionSpec partition_spec = 4;</code>
     */
    public com.google.cloud.asset.v1.PartitionSpec.Builder getPartitionSpecBuilder() {

      onChanged();
      return getPartitionSpecFieldBuilder().getBuilder();
    }
    /**
     *
     *
     * <pre>
     * [partition_spec] determines whether to export to partitioned table(s) and
     * how to partition the data.
     * If [partition_spec] is unset or [partition_spec.partition_key] is unset or
     * `PARTITION_KEY_UNSPECIFIED`, the snapshot results will be exported to
     * non-partitioned table(s). [force] will decide whether to overwrite existing
     * table(s).
     * If [partition_spec] is specified. First, the snapshot results will be
     * written to partitioned table(s) with two additional timestamp columns,
     * readTime and requestTime, one of which will be the partition key. Secondly,
     * in the case when any destination table already exists, it will first try to
     * update existing table's schema as necessary by appending additional
     * columns. Then, if [force] is `TRUE`, the corresponding partition will be
     * overwritten by the snapshot results (data in different partitions will
     * remain intact); if [force] is unset or `FALSE`, it will append the data. An
     * error will be returned if the schema update or data appension fails.
     * </pre>
     *
     * <code>.google.cloud.asset.v1.PartitionSpec partition_spec = 4;</code>
     */
    public com.google.cloud.asset.v1.PartitionSpecOrBuilder getPartitionSpecOrBuilder() {
      if (partitionSpecBuilder_ != null) {
        return partitionSpecBuilder_.getMessageOrBuilder();
      } else {
        return partitionSpec_ == null
            ? com.google.cloud.asset.v1.PartitionSpec.getDefaultInstance()
            : partitionSpec_;
      }
    }
    /**
     *
     *
     * <pre>
     * [partition_spec] determines whether to export to partitioned table(s) and
     * how to partition the data.
     * If [partition_spec] is unset or [partition_spec.partition_key] is unset or
     * `PARTITION_KEY_UNSPECIFIED`, the snapshot results will be exported to
     * non-partitioned table(s). [force] will decide whether to overwrite existing
     * table(s).
     * If [partition_spec] is specified. First, the snapshot results will be
     * written to partitioned table(s) with two additional timestamp columns,
     * readTime and requestTime, one of which will be the partition key. Secondly,
     * in the case when any destination table already exists, it will first try to
     * update existing table's schema as necessary by appending additional
     * columns. Then, if [force] is `TRUE`, the corresponding partition will be
     * overwritten by the snapshot results (data in different partitions will
     * remain intact); if [force] is unset or `FALSE`, it will append the data. An
     * error will be returned if the schema update or data appension fails.
     * </pre>
     *
     * <code>.google.cloud.asset.v1.PartitionSpec partition_spec = 4;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
            com.google.cloud.asset.v1.PartitionSpec,
            com.google.cloud.asset.v1.PartitionSpec.Builder,
            com.google.cloud.asset.v1.PartitionSpecOrBuilder>
        getPartitionSpecFieldBuilder() {
      if (partitionSpecBuilder_ == null) {
        partitionSpecBuilder_ =
            new com.google.protobuf.SingleFieldBuilderV3<
                com.google.cloud.asset.v1.PartitionSpec,
                com.google.cloud.asset.v1.PartitionSpec.Builder,
                com.google.cloud.asset.v1.PartitionSpecOrBuilder>(
                getPartitionSpec(), getParentForChildren(), isClean());
        partitionSpec_ = null;
      }
      return partitionSpecBuilder_;
    }

    private boolean separateTablesPerAssetType_;
    /**
     *
     *
     * <pre>
     * If this flag is `TRUE`, the snapshot results will be written to one or
     * multiple tables, each of which contains results of one asset type. The
     * [force] and [partition_spec] fields will apply to each of them.
     * Field [table] will be concatenated with "_" and the asset type names (see
     * https://cloud.google.com/asset-inventory/docs/supported-asset-types for
     * supported asset types) to construct per-asset-type table names, in which
     * all non-alphanumeric characters like "." and "/" will be substituted by
     * "_". Example: if field [table] is "mytable" and snapshot results
     * contain "storage.googleapis.com/Bucket" assets, the corresponding table
     * name will be "mytable_storage_googleapis_com_Bucket". If any of these
     * tables does not exist, a new table with the concatenated name will be
     * created.
     * When [content_type] in the ExportAssetsRequest is `RESOURCE`, the schema of
     * each table will include RECORD-type columns mapped to the nested fields in
     * the Asset.resource.data field of that asset type (up to the 15 nested level
     * BigQuery supports
     * (https://cloud.google.com/bigquery/docs/nested-repeated#limitations)). The
     * fields in &gt;15 nested levels will be stored in JSON format string as a child
     * column of its parent RECORD column.
     * If error occurs when exporting to any table, the whole export call will
     * return an error but the export results that already succeed will persist.
     * Example: if exporting to table_type_A succeeds when exporting to
     * table_type_B fails during one export call, the results in table_type_A will
     * persist and there will not be partial results persisting in a table.
     * </pre>
     *
     * <code>bool separate_tables_per_asset_type = 5;</code>
     *
     * @return The separateTablesPerAssetType.
     */
    @java.lang.Override
    public boolean getSeparateTablesPerAssetType() {
      return separateTablesPerAssetType_;
    }
    /**
     *
     *
     * <pre>
     * If this flag is `TRUE`, the snapshot results will be written to one or
     * multiple tables, each of which contains results of one asset type. The
     * [force] and [partition_spec] fields will apply to each of them.
     * Field [table] will be concatenated with "_" and the asset type names (see
     * https://cloud.google.com/asset-inventory/docs/supported-asset-types for
     * supported asset types) to construct per-asset-type table names, in which
     * all non-alphanumeric characters like "." and "/" will be substituted by
     * "_". Example: if field [table] is "mytable" and snapshot results
     * contain "storage.googleapis.com/Bucket" assets, the corresponding table
     * name will be "mytable_storage_googleapis_com_Bucket". If any of these
     * tables does not exist, a new table with the concatenated name will be
     * created.
     * When [content_type] in the ExportAssetsRequest is `RESOURCE`, the schema of
     * each table will include RECORD-type columns mapped to the nested fields in
     * the Asset.resource.data field of that asset type (up to the 15 nested level
     * BigQuery supports
     * (https://cloud.google.com/bigquery/docs/nested-repeated#limitations)). The
     * fields in &gt;15 nested levels will be stored in JSON format string as a child
     * column of its parent RECORD column.
     * If error occurs when exporting to any table, the whole export call will
     * return an error but the export results that already succeed will persist.
     * Example: if exporting to table_type_A succeeds when exporting to
     * table_type_B fails during one export call, the results in table_type_A will
     * persist and there will not be partial results persisting in a table.
     * </pre>
     *
     * <code>bool separate_tables_per_asset_type = 5;</code>
     *
     * @param value The separateTablesPerAssetType to set.
     * @return This builder for chaining.
     */
    public Builder setSeparateTablesPerAssetType(boolean value) {

      separateTablesPerAssetType_ = value;
      onChanged();
      return this;
    }
    /**
     *
     *
     * <pre>
     * If this flag is `TRUE`, the snapshot results will be written to one or
     * multiple tables, each of which contains results of one asset type. The
     * [force] and [partition_spec] fields will apply to each of them.
     * Field [table] will be concatenated with "_" and the asset type names (see
     * https://cloud.google.com/asset-inventory/docs/supported-asset-types for
     * supported asset types) to construct per-asset-type table names, in which
     * all non-alphanumeric characters like "." and "/" will be substituted by
     * "_". Example: if field [table] is "mytable" and snapshot results
     * contain "storage.googleapis.com/Bucket" assets, the corresponding table
     * name will be "mytable_storage_googleapis_com_Bucket". If any of these
     * tables does not exist, a new table with the concatenated name will be
     * created.
     * When [content_type] in the ExportAssetsRequest is `RESOURCE`, the schema of
     * each table will include RECORD-type columns mapped to the nested fields in
     * the Asset.resource.data field of that asset type (up to the 15 nested level
     * BigQuery supports
     * (https://cloud.google.com/bigquery/docs/nested-repeated#limitations)). The
     * fields in &gt;15 nested levels will be stored in JSON format string as a child
     * column of its parent RECORD column.
     * If error occurs when exporting to any table, the whole export call will
     * return an error but the export results that already succeed will persist.
     * Example: if exporting to table_type_A succeeds when exporting to
     * table_type_B fails during one export call, the results in table_type_A will
     * persist and there will not be partial results persisting in a table.
     * </pre>
     *
     * <code>bool separate_tables_per_asset_type = 5;</code>
     *
     * @return This builder for chaining.
     */
    public Builder clearSeparateTablesPerAssetType() {

      separateTablesPerAssetType_ = false;
      onChanged();
      return this;
    }

    @java.lang.Override
    public final Builder setUnknownFields(final com.google.protobuf.UnknownFieldSet unknownFields) {
      return super.setUnknownFields(unknownFields);
    }

    @java.lang.Override
    public final Builder mergeUnknownFields(
        final com.google.protobuf.UnknownFieldSet unknownFields) {
      return super.mergeUnknownFields(unknownFields);
    }

    // @@protoc_insertion_point(builder_scope:google.cloud.asset.v1.BigQueryDestination)
  }

  // @@protoc_insertion_point(class_scope:google.cloud.asset.v1.BigQueryDestination)
  private static final com.google.cloud.asset.v1.BigQueryDestination DEFAULT_INSTANCE;

  static {
    DEFAULT_INSTANCE = new com.google.cloud.asset.v1.BigQueryDestination();
  }

  public static com.google.cloud.asset.v1.BigQueryDestination getDefaultInstance() {
    return DEFAULT_INSTANCE;
  }

  private static final com.google.protobuf.Parser<BigQueryDestination> PARSER =
      new com.google.protobuf.AbstractParser<BigQueryDestination>() {
        @java.lang.Override
        public BigQueryDestination parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new BigQueryDestination(input, extensionRegistry);
        }
      };

  public static com.google.protobuf.Parser<BigQueryDestination> parser() {
    return PARSER;
  }

  @java.lang.Override
  public com.google.protobuf.Parser<BigQueryDestination> getParserForType() {
    return PARSER;
  }

  @java.lang.Override
  public com.google.cloud.asset.v1.BigQueryDestination getDefaultInstanceForType() {
    return DEFAULT_INSTANCE;
  }
}
